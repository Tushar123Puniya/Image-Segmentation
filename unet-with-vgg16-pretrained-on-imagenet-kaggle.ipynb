{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nimport torchvision.models as model\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport torchvision\nfrom torch.utils.data import DataLoader\nimport torch\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nfrom numpy import random\n# !pip install focal_loss_torch\n# from focal_loss.focal_loss import FocalLoss\nimport numpy as np\nfrom mpl_toolkits import mplot3d\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-poster')\n\n# \nfrom torchsummary import summary\n","metadata":{"id":"PqL9L0eSTdax","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2771f94e-b48d-4389-b239-084ed0287a13","execution":{"iopub.status.busy":"2023-08-25T20:07:40.960214Z","iopub.execute_input":"2023-08-25T20:07:40.961187Z","iopub.status.idle":"2023-08-25T20:07:48.213826Z","shell.execute_reply.started":"2023-08-25T20:07:40.961139Z","shell.execute_reply":"2023-08-25T20:07:48.211269Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/tmp/ipykernel_32/581125780.py:25: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-poster')\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseaborn-poster\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# !pip install torchsummary\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"],"ename":"ModuleNotFoundError","evalue":"No module named 'torchsummary'","output_type":"error"}]},{"cell_type":"code","source":"!mkdir output","metadata":{"execution":{"iopub.status.busy":"2023-08-25T20:07:48.214962Z","iopub.status.idle":"2023-08-25T20:07:48.215962Z","shell.execute_reply.started":"2023-08-25T20:07:48.215659Z","shell.execute_reply":"2023-08-25T20:07:48.215688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models import vgg13_bn, vgg16_bn\n\n__all__ = ['vgg13bn_unet', 'vgg16bn_unet']\n\n\ndef double_conv(in_channels, out_channels):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef up_conv(in_channels, out_channels):\n    return nn.ConvTranspose2d(\n        in_channels, out_channels, kernel_size=2, stride=2\n    )\n\n\nclass VGGUnet(nn.Module):\n    \"\"\"Unet with VGG-13 (with BN), VGG-16 (with BN) encoder.\n    \"\"\"\n\n    def __init__(self,encoder,pretrained=True, out_channels=1):\n        super().__init__()\n\n        self.encoder = vgg16_bn(pretrained=True).features\n        for param in self.encoder.parameters():\n          param.requires_grad=False\n\n        self.block1 = nn.Sequential(*self.encoder[:6])\n        self.block2 = nn.Sequential(*self.encoder[6:13])\n        self.block3 = nn.Sequential(*self.encoder[13:20])\n        self.block4 = nn.Sequential(*self.encoder[20:27])\n        self.block5 = nn.Sequential(*self.encoder[27:34])\n\n        self.bottleneck = nn.Sequential(*self.encoder[34:])\n        self.conv_bottleneck = double_conv(512, 1024)\n\n        self.up_conv6 = up_conv(1024, 512)\n        self.conv6 = double_conv(512 + 512, 512)\n        self.up_conv7 = up_conv(512, 256)\n        self.conv7 = double_conv(256 + 512, 256)\n        self.up_conv8 = up_conv(256, 128)\n        self.conv8 = double_conv(128 + 256, 128)\n        self.up_conv9 = up_conv(128, 64)\n        self.conv9 = double_conv(64 + 128, 64)\n        self.up_conv10 = up_conv(64, 32)\n        self.conv10 = double_conv(32 + 64, 32)\n        self.conv11 = nn.Conv2d(32, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        block1 = self.block1(x)\n        block2 = self.block2(block1)\n        block3 = self.block3(block2)\n        block4 = self.block4(block3)\n        block5 = self.block5(block4)\n\n        bottleneck = self.bottleneck(block5)\n        x = self.conv_bottleneck(bottleneck)\n\n        x = self.up_conv6(x)\n        if x.shape != block5.shape:\n          x = TF.resize(x,size = block5.shape[2:])\n\n        x = torch.cat([x, block5], dim=1)\n        x = self.conv6(x)\n\n        x = self.up_conv7(x)\n        if x.shape != block4.shape:\n          x = TF.resize(x,size = block4.shape[2:])\n\n        x = torch.cat([x, block4], dim=1)\n        x = self.conv7(x)\n\n        x = self.up_conv8(x)\n        if x.shape != block3.shape:\n          x = TF.resize(x,size = block3.shape[2:])\n\n        x = torch.cat([x, block3], dim=1)\n        x = self.conv8(x)\n\n        x = self.up_conv9(x)\n        if x.shape != block2.shape:\n          x = TF.resize(x,size = block2.shape[2:])\n\n        x = torch.cat([x, block2], dim=1)\n        x = self.conv9(x)\n\n        x = self.up_conv10(x)\n        if x.shape != block1.shape:\n          x = TF.resize(x,size = block1.shape[2:])\n\n        x = torch.cat([x, block1], dim=1)\n        x = self.conv10(x)\n\n        x = self.conv11(x)\n\n        return x\n\n\ndef vgg13bn_unet(output_dim: int=2, pretrained: bool=False):\n    return VGGUnet(vgg13_bn, pretrained=pretrained, out_channels=output_dim)\n\n\ndef vgg16bn_unet(output_dim: int=1, pretrained: bool=True):\n    return VGGUnet(vgg16_bn, pretrained=pretrained, out_channels=output_dim)","metadata":{"id":"qEiQdVDdToH9","execution":{"iopub.status.busy":"2023-08-25T20:07:48.217810Z","iopub.status.idle":"2023-08-25T20:07:48.218366Z","shell.execute_reply.started":"2023-08-25T20:07:48.218077Z","shell.execute_reply":"2023-08-25T20:07:48.218102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SlumDataset(Dataset):\n  def __init__(self,image_dir,mask_dir,transform = None):\n    self.image_dir = image_dir\n    self.mask_dir = mask_dir\n    self.transform = transform\n    self.images = os.listdir(image_dir)\n    self.mask = os.listdir(mask_dir)\n\n  def __len__(self):\n    return len(self.mask)\n\n  def __getitem__(self,index):\n    img_path = os.path.join(self.image_dir,self.images[index])\n    mask_path = os.path.join(self.mask_dir,self.mask[index])\n\n    # img_path = self.image_dir[index]\n    # mask_path = self.mask_dir[index]\n\n    image = np.array(Image.open(img_path))\n    mask = np.array(Image.open(mask_path), dtype = np.float32)\n    # mask = np.array(Image.open(mask_path))\n\n    mask = (mask//246.0)\n\n    if self.transform != None:\n      augmentations = self.transform(image = image,mask = mask)\n      image = augmentations[\"image\"]\n      mask = augmentations[\"mask\"]\n\n    return image,mask\n\n# def distributeData(train_image,train_mask,train_val):\n#     images = os.listdir(train_image)\n#     masks = os.listdir(train_mask)\n\n#     for i in range(0,len(images)):\n#       images[i] = os.path.join(train_image,images[i])\n\n#     for i in range(0,len(masks)):\n#       masks[i] = os.path.join(train_mask,masks[i])\n\n\n#     train_img, valid_img = images[:int(train_val*len(images))], images[int(train_val*len(images)):]\n#     train_mask,valid_mask = masks[:int(train_val*len(masks))],masks[int(train_val*len(masks)):]\n\n#     return train_img,train_mask,valid_img,valid_mask\n\ndef save_checkpoint(state,filename = \"my_checkpoint.pth.tar\"):\n  print(\"=> Saving checkpoint\")\n  torch.save(state,filename)\n\ndef load_checkpoint(checkpoint,model,optimizer):\n  print(\"=> Loading checkpoint\")\n  model.load_state_dict(checkpoint[\"state_dict\"])\n  optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\ndef check_accuracy(loader,model,accuracy_vals,device = \"cuda\"):\n  num_correct = 0\n  num_pixels = 0\n  dice_score = 0\n  model.eval()\n\n  with torch.no_grad():\n    for x,y in loader:\n      x = x.to(device)\n      y = y.to(device).unsqueeze(1)\n      preds = torch.sigmoid(model(x))\n\n      preds = (preds > 0.5).float()\n      num_correct += (preds == y).sum()\n      num_pixels += torch.numel(preds)\n      dice_score += (2*(preds*y).sum())/((preds+y).sum() + 1e-8)\n\n  accu = (num_correct/num_pixels)*100\n  print(f\"Got {num_correct}/{num_pixels} with accuracy {accu}\")\n\n  # index = accu.cpu().data.numpy().argmax()\n  print(accu.item())\n  accuracy_vals.append(accu.item())\n\n  print(f\"Dice score: {dice_score/len(loader)}\")\n  model.train()\n\ndef save_predictions_as_imgs(loader,model,folder,device = \"cuda\"):\n  model.eval()\n  for idx, (x,y) in enumerate(loader):\n    x = x.to(device = device)\n\n    with torch.no_grad():\n      preds = torch.sigmoid(model(x))\n      preds = (preds > 0.5).float()\n\n    torchvision.utils.save_image(preds,f\"{folder}/pred_{idx}.jpg\")\n    torchvision.utils.save_image(y.unsqueeze(1),f\"{folder}/{idx}.jpg\")\n\n  model.train()\n\n\ndef my_plot(epochs, loss):\n  plt.xlabel(\"losses\")\n  plt.ylabel(\"Num of epochs\")\n  plt.plot(epochs, loss)","metadata":{"id":"R_l9yEDPTuAy","execution":{"iopub.status.busy":"2023-08-25T20:07:48.220098Z","iopub.status.idle":"2023-08-25T20:07:48.220649Z","shell.execute_reply.started":"2023-08-25T20:07:48.220373Z","shell.execute_reply":"2023-08-25T20:07:48.220400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_loaders(\n    train_dir,\n    train_mask_dir,\n    val_dir,\n    val_mask_dir,\n    batch_size,\n    train_transform,\n    val_transform,\n    train_val,\n    num_workers = 2,\n    pin_memory = True,\n):\n\n  # train_img,train_mask,val_img,val_mask = distributeData(train_dir,train_maskdir,train_val)\n\n  train_ds = SlumDataset(\n      image_dir = train_dir,\n      mask_dir = train_mask_dir,\n      transform = train_transform\n  )\n\n  train_loader = DataLoader(\n      train_ds,\n      batch_size = batch_size,\n      num_workers = num_workers,\n      pin_memory = pin_memory,\n      shuffle = True,\n  )\n\n  val_ds = SlumDataset(\n      image_dir = val_dir,\n      mask_dir = val_mask_dir,\n      transform = val_transform\n  )\n\n  val_loader = DataLoader(\n      val_ds,\n      batch_size = batch_size,\n      num_workers = num_workers,\n      pin_memory = pin_memory,\n      shuffle = False,\n  )\n\n  return train_loader, val_loader\n","metadata":{"id":"Yuk_bzgBT8fI","execution":{"iopub.status.busy":"2023-08-25T20:07:48.221792Z","iopub.status.idle":"2023-08-25T20:07:48.222342Z","shell.execute_reply.started":"2023-08-25T20:07:48.222052Z","shell.execute_reply":"2023-08-25T20:07:48.222077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset (BTP 1 and 2)\n# Hyperparameters\n# LEARNING_RATE = [1e-4,1e-3,1e-2]\nLEARNING_RATE = 1e-4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\nNUM_EPOCHS = 50\nNUM_WORKERS = 2\ntrain_val = 0.8\nIMAGE_HEIGHT = 210\nIMAGE_WIDTH = 210\ntrain_valid = 0.8\nPIN_MEMORY = True\nLOAD_MODEL = True\nTRAIN_IMG_DIR = '/kaggle/input/slum-dataset/train_img/train_img'\nTRAIN_MASK_DIR = '/kaggle/input/slum-dataset/train_mask-20230822T130205Z-001/train_mask'\nval_dir = '/kaggle/input/slum-dataset/val_img-20230822T130211Z-001/val_img'\nval_mask_dir = '/kaggle/input/slum-dataset/val_mask-20230822T130216Z-001/val_mask'\nsaved_folder = '//kaggle/working/'","metadata":{"id":"0vG4QB5rh5Y9","execution":{"iopub.status.busy":"2023-08-25T20:07:48.223921Z","iopub.status.idle":"2023-08-25T20:07:48.225087Z","shell.execute_reply.started":"2023-08-25T20:07:48.224791Z","shell.execute_reply":"2023-08-25T20:07:48.224819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\ndef train_fn(epoch,num_epochs,loader,model,optimizer,loss_fn,scaler,iou_train,precision_train,recall_train,f1_train,loss_train,accuracy_train):\n  total_loss = 0.0\n  total_iou = 0.0\n  total_precision = 0.0\n  total_recall = 0.0\n  total_f1 = 0.0\n  total_accuracy = 0.0\n\n  length = len(loader);\n  loop = tqdm(loader)\n\n  for batch_idx, (data,targets) in enumerate(loop):\n    data = data.to(device = device)\n    targets = targets.float().unsqueeze(1).to(device = device)\n    targ = targets\n\n    # forward\n    with torch.cuda.amp.autocast():\n      predictions = torch.sigmoid(model(data))\n\n      loss = loss_fn(predictions,targets)\n      total_loss += loss.item()\n\n      # convert model outputs to binary mask using sigmoid and threshold\n      predicted_masks = predictions\n      predicted_masks = (predicted_masks > 0.5).float()\n\n      preds = predicted_masks.cpu().numpy()\n      yy = targ.cpu().numpy()\n\n      # Calculate the intersection and union of the binary masks\n      intersection = np.sum(preds * yy)\n      union = np.sum(np.logical_or(preds, yy))\n      iou = intersection / union\n\n      precision = precision_score(yy.flatten(), preds.flatten())\n      recall = recall_score(yy.flatten(), preds.flatten())\n      f1 = f1_score(yy.flatten(), preds.flatten())\n      accuracy = np.mean(yy.flatten() == preds.flatten())\n\n      total_iou += iou\n      total_precision += precision\n      total_recall += recall\n      total_f1 += f1\n      total_accuracy += accuracy\n\n    # backward\n    optimizer.zero_grad()\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n    # update tqdm loop\n    loop.set_postfix(loss = loss.item())\n\n  average_loss = total_loss / length\n  average_iou = total_iou / length\n  average_precision = total_precision / length\n  average_recall = total_recall / length\n  average_f1 = total_f1 / length\n  average_accuracy = accuracy /length\n\n  # Append metrics and losses to lists for plotting\n  loss_train.append(average_loss)\n  iou_train.append(average_iou)\n  precision_train.append(average_precision)\n  recall_train.append(average_recall)\n  f1_train.append(average_f1)\n  accuracy_train.append(average_accuracy)\n\n  print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n          f\"Loss: {average_loss:.4f}, \"\n          f\"IoU: {average_iou:.4f}, \"\n          f\"Precision: {average_precision:.4f}, \"\n          f\"Recall: {average_recall:.4f}, \"\n          f\"F1: {average_f1:.4f},\"\n          f\"Accuracy: {average_accuracy:4f}\"\n  )","metadata":{"id":"ZQu0VIotmmGI","execution":{"iopub.status.busy":"2023-08-25T20:07:48.226922Z","iopub.status.idle":"2023-08-25T20:07:48.227469Z","shell.execute_reply.started":"2023-08-25T20:07:48.227181Z","shell.execute_reply":"2023-08-25T20:07:48.227205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_coefficient(predicted_mask, ground_truth_mask):\n  intersection = np.sum(predicted_mask * ground_truth_mask)\n  predicted_area = np.sum(predicted_mask)\n  ground_truth_area = np.sum(ground_truth_mask)\n\n  dice = (2.0 * intersection) / (predicted_area + ground_truth_area)\n  return dice\n\ndef check_accuracy(loader,model,accuracy_vals,dice_scores,iou_scores,precision_scores,recall_scores,f1_val,device = \"cuda\"):\n  num_correct = 0\n  num_pixels = 0\n  dice_s = []\n  precision_s = []\n  f1_s = []\n  iou_s = []\n  recall_s = []\n  accuracy_s = []\n  model.eval()\n\n  with torch.no_grad():\n    for x,y in loader:\n      x = x.to(device)\n      y = y.to(device).unsqueeze(1)\n      pred = torch.sigmoid(model(x))\n\n      pred = (pred > 0.5).float()\n      num_correct += (pred == y).sum()\n      num_pixels += torch.numel(pred)\n      preds = pred.cpu().numpy()\n      yy = y.cpu().numpy()\n\n      for pred_mask, gt_mask in zip(preds, yy):\n        # Flatten the binary masks for precision calculation\n        pred_mask_flat = pred_mask.flatten()\n        gt_mask_flat = gt_mask.flatten()\n\n        # Calculate recall score for the current mask\n        recall = recall_score(gt_mask_flat, pred_mask_flat)\n        recall_s.append(recall)\n\n        # Calculate precision for the current mask\n        precision = precision_score(gt_mask_flat, pred_mask_flat)\n        precision_s.append(precision)\n\n        # Calculate F1 score for the current mask\n        f1 = f1_score(gt_mask_flat, pred_mask_flat)\n        f1_s.append(f1)\n\n        # Calculate the intersection and union of the binary masks\n        intersection = np.sum(pred_mask * gt_mask)\n        union = np.sum(np.logical_or(pred_mask, gt_mask))\n\n        # Calculate IoU for the current mask\n        iou = intersection / union\n        iou_s.append(iou)\n\n        # Calculate Accuarcy for the current mask\n        accuracy = np.mean(gt_mask_flat == pred_mask_flat)\n        accuracy_s.append(accuracy)\n\n        dice_score = dice_coefficient(pred_mask, gt_mask)\n        dice_s.append(dice_score)\n\n  accu = (num_correct/num_pixels)*100\n  print(f\"Got {num_correct}/{num_pixels} with accuracy {accu}\")\n\n  # Calculate the average precision over the validation dataset\n  average_precision = np.mean(precision_s)\n  print(f\"Average Precision: {average_precision:.4f}\")\n\n  #Calculate the average accuracy over the validation dataset\n  accuracy_vals.append(np.mean(accuracy_s))\n  print(f'Accuracy: {np.mean(accuracy_s)}')\n\n  # Calculate the average Dice score over the validation dataset\n  average_dice = np.mean(dice_s)\n  print(f\"Average Dice Score: {average_dice:.4f}\")\n\n  # Calculate the average F1 score over the validation dataset\n  average_f1 = np.mean(f1_s)\n  print(f\"Average F1 Score: {average_f1:.4f}\")\n\n  # Calculate the average IoU score over the validation dataset\n  average_iou = np.mean(iou_s)\n  print(f\"Average IoU Score: {average_iou:.4f}\")\n\n  # Calculate average metrics for the epoch\n  iou_scores.append(average_iou)\n  precision_scores.append(average_precision)\n  recall_scores.append(np.mean(recall_s))\n  f1_val.append(average_f1)\n  dice_scores.append(average_dice)\n\n  model.train()","metadata":{"id":"5TwKd88PoEa0","execution":{"iopub.status.busy":"2023-08-25T20:07:48.228988Z","iopub.status.idle":"2023-08-25T20:07:48.229551Z","shell.execute_reply.started":"2023-08-25T20:07:48.229244Z","shell.execute_reply":"2023-08-25T20:07:48.229270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def func(NUM_EPOCHS,loss_train,iou_train,precision_train,recall_train,f1_train,iou_scores,precision_scores,recall_scores, f1_val,dice_scores, accuracy_vals)\n  epochs = np.arange(1, NUM_EPOCHS + 1)\n  plt.figure(figsize=(12, 8))\n\n  plt.plot(epochs, loss_train, label='Train_Loss')\n  plt.plot(epochs, iou_train, label='Train_IoU')\n  plt.plot(epochs, precision_train, label='Train_Precision')\n  plt.plot(epochs, recall_train, label='Train_Recall')\n  plt.plot(epochs, f1_train, label='Train_F1')\n  # plt.plot(epochs, accuracy_train, label='Train_Accuracy')\n\n  plt.xlabel('Train_Epoch')\n  plt.ylabel('Score / Loss')\n  plt.title('Training Metrics and Losses Over Epochs')\n  plt.legend()\n  plt.grid()\n\n  plt.show()\n\n  # PLot validation metrics\n  plt.figure(figsize=(12, 8))\n\n  plt.plot(epochs, iou_scores, label='validation_IoU')\n  plt.plot(epochs, precision_scores, label='validation_Precision')\n  plt.plot(epochs, recall_scores, label='validation_Recall')\n  plt.plot(epochs, f1_val, label='validation_F1')\n  plt.plot(epochs, dice_scores, label='validation_Dice')\n  plt.plot(epochs,accuracy_vals,label = \"validation_Accuracy\")\n\n  plt.xlabel('validation_Epoch')\n  plt.ylabel('Score')\n  plt.title('Validation Metrics Over Epochs')\n  plt.legend()\n  plt.grid()\n\n  plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\ndef main():\n  # data augumentation:\n  train_transform = A.Compose(\n    [\n      A.Resize(height = IMAGE_HEIGHT,width = IMAGE_WIDTH),\n      A.Rotate(limit = 35,p=1.0),\n      A.HorizontalFlip(p=0.1),\n      A.VerticalFlip(p=0.1),\n      A.Normalize(\n          mean=[0.0,0.0,0.0],\n          std = [1.0,1.0,1.0],\n          max_pixel_value = 255.0\n      ),\n      ToTensorV2(),\n    ],is_check_shapes = False\n  )\n\n  val_transform = A.Compose(\n    [\n     A.Resize(height = IMAGE_HEIGHT,width = IMAGE_WIDTH),\n     A.Normalize(\n      mean=[0.0,0.0,0.0],\n      std = [1.0,1.0,1.0],\n      max_pixel_value = 255.0\n    ),\n    ToTensorV2(),\n  ],is_check_shapes = False\n  )\n\n  # model instance:\n  model = vgg16bn_unet().to(DEVICE)\n\n  # defining loss:\n  loss_fn = nn.BCEWithLogitsLoss()\n\n  #defining optimizer:\n  optimizer = optim.Adam(model.parameters(),lr = LEARNING_RATE)\n  scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n\n  # setting loaders:\n  train_loader,val_loader = get_loaders(\n      TRAIN_IMG_DIR,\n      TRAIN_MASK_DIR,\n      val_dir,\n      val_mask_dir,\n      BATCH_SIZE,\n      train_transform,\n      val_transform,\n      train_val,\n      NUM_WORKERS,\n      PIN_MEMORY,\n  )\n\n  # if LOAD_MODEL :\n  #   load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"),model,optimizer=optimizer)\n\n  # check_accuracy(val_loader,model,device = DEVICE)\n\n  scaler = torch.cuda.amp.GradScaler()\n\n\n# Initialize lists to store metric values for each epoch for validation dataset\n  iou_scores = []\n  precision_scores = []\n  recall_scores = []\n  f1_val = []\n  dice_scores = []\n  accuracy_vals = []\n\n # Initialize lists to store metric values for each epoch for training dataset\n  iou_train = []\n  precision_train = []\n  recall_train = []\n  f1_train = []\n  loss_train = []\n  accuracy_train = []\n\n\n  for epoch in range(NUM_EPOCHS):\n    scheduler.step()\n    train_fn(epoch,NUM_EPOCHS,train_loader,model,optimizer,loss_fn,scaler,iou_train,precision_train,recall_train,f1_train,loss_train,accuracy_train)\n\n    # save model\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    save_checkpoint(checkpoint)\n\n    # check_accuracy\n    val_loss = 0.0\n    check_accuracy(val_loader,model,accuracy_vals,dice_scores,iou_scores,precision_scores,recall_scores,f1_val,device = DEVICE)\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered.\")\n            save_predictions_as_imgs(\n            val_loader,model,folder=saved_folder,device = DEVICE\n            )\n            func(epoch,loss_train,iou_train,precision_train,recall_train,f1_train,iou_scores,precision_scores,recall_scores, f1_val,dice_scores, accuracy_vals)\n            break\n            \n    # print some examples to the folder\n    save_predictions_as_imgs(\n        val_loader,model,folder = saved_folder,device = DEVICE\n    )\n  print(\"Training finished\")\n\n  # Plot metrics and losses\n  # Plot for Training dataset\n  func(NUM_EPOCHS,loss_train,iou_train,precision_train,recall_train,f1_train,iou_scores,precision_scores,recall_scores, f1_val,dice_scores, accuracy_vals)\n\nif __name__ == \"__main__\":\n  main()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Ru0ZAQB0e1s","outputId":"8b3a220e-93a7-47d3-9c2a-3233765f1a10","execution":{"iopub.status.busy":"2023-08-25T20:07:48.231234Z","iopub.status.idle":"2023-08-25T20:07:48.233457Z","shell.execute_reply.started":"2023-08-25T20:07:48.233157Z","shell.execute_reply":"2023-08-25T20:07:48.233184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_f(train_loader,val_loader,model,optimizer,loss_fn,scaler,saved_folder):\n  model.to(device)\n  loop = tqdm(train_loader)\n\n  train_losses = []\n  train_accuracies = []\n  val_accuracies = []\n\n  for epoch in range(NUM_EPOCHS):\n    model.train()\n    running_loss = 0.0\n    total_train_accuracy = 0.0\n\n    for batch_idx, (data,targets) in enumerate(loop):\n      data = data.to(device = device)\n      targets = targets.float().unsqueeze(1).to(device = device)\n\n      # forward\n      with torch.cuda.amp.autocast():\n        predictions = torch.sigmoid(model(data))\n\n        loss = criterion(predictions,targets)\n\n      # backward\n      optimizer.zero_grad()\n      scaler.scale(loss).backward()\n      scaler.step(optimizer)\n      scaler.update()\n\n      # update tqdm loop\n      loop.set_postfix(loss = loss.item())\n      running_loss += loss.item()\n      accuracy,_,_ = compute_metrics(predictions, targets)\n      total_train_accuracy += accuracy\n\n    epoch_train_loss = running_loss / len(train_loader)\n    epoch_train_accuracy = total_train_accuracy / len(train_loader)\n\n    # Validation phase\n    model.eval()\n    total_val_accuracy = 0.0\n    with torch.no_grad():\n        for idx,(val_inputs, val_targets) in enumerate(tqdm(val_loader)):\n            # val_inputs, val_targets = val_inputs.to(device), val_targets.float().unsqueeze(1).to(device = device)\n            val_inputs = val_inputs.to(device)\n            # val_targets = val_targets.to(device);\n            val_targets = val_targets.float().unsqueeze(1).to(device);\n\n            with torch.no_grad():\n              val_outputs = torch.sigmoid(model(val_inputs))\n\n            # saving the predicted images\n            torchvision.utils.save_image(val_outputs,f\"{saved_folder}/pred_{idx}.jpg\")\n            torchvision.utils.save_image(val_targets,f\"{saved_folder}/{idx}.jpg\")\n\n            val_accuracy, _, _ = compute_metrics(val_outputs, val_targets)\n            total_val_accuracy += val_accuracy\n\n    epoch_val_accuracy = total_val_accuracy / len(val_loader)\n\n    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] - Train Loss: {epoch_train_loss:.4f} - \"\n              f\"Train Accuracy: {epoch_train_accuracy:.4f} - Val Accuracy: {epoch_val_accuracy:.4f}\")\n\n    train_losses.append(epoch_train_loss)\n    train_accuracies.append(epoch_train_accuracy)\n    val_accuracies.append(epoch_val_accuracy)\n\n  print(\"Training finished.\")\n\n  # Save the trained model\n  torch.save(model.state_dict(), \"trained_model.pth\")\n  print(\"Model saved.\")\n\n  # Plot accuracy graph\n  plt.figure()\n  plt.plot(train_accuracies, label='Train Accuracy')\n  plt.plot(val_accuracies, label='Validation Accuracy')\n  plt.xlabel('Epoch')\n  plt.ylabel('Accuracy')\n  plt.title('Accuracy vs. Epoch')\n  plt.legend()\n  plt.show()\n\n\nif __name__ == \"__main__\":\n    # Initialize your model, optimizer, criterion, datasets, and dataloaders\n    model = vgg16bn_unet().to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(),lr = LEARNING_RATE)\n\n    train_transform = A.Compose(\n    [\n      A.Resize(height = IMAGE_HEIGHT,width = IMAGE_WIDTH),\n      A.Rotate(limit = 35,p=1.0),\n      A.HorizontalFlip(p=0.1),\n      A.VerticalFlip(p=0.1),\n      A.Normalize(\n          mean=[0.0,0.0,0.0],\n          std = [1.0,1.0,1.0],\n          max_pixel_value = 255.0\n      ),\n      ToTensorV2(),\n    ],\n  )\n\n    val_transform = A.Compose(\n      [\n       A.Resize(height = IMAGE_HEIGHT,width = IMAGE_WIDTH),\n       A.Normalize(\n        mean=[0.0,0.0,0.0],\n        std = [1.0,1.0,1.0],\n        max_pixel_value = 255.0\n      ),\n      ToTensorV2(),\n    ]\n    )\n\n    train_loader,val_loader = get_loaders(\n      TRAIN_IMG_DIR,\n      TRAIN_MASK_DIR,\n      BATCH_SIZE,\n      train_transform,\n      val_transform,\n      train_val,\n      NUM_WORKERS,\n      PIN_MEMORY,\n  )\n    saved_folder = '/content/gdrive/MyDrive/Data/Updated Annotated data/1 btp/1/saved images'\n\n    scaler = torch.cuda.amp.GradScaler()\n\n    train_fn(train_loader,val_loader,model,optimizer,criterion,scaler,saved_folder)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"6Rp2IvZvVg3-","outputId":"be19b46b-d44c-4cf2-e2c7-bfebbe7c8579","execution":{"iopub.status.busy":"2023-08-25T20:07:48.234889Z","iopub.status.idle":"2023-08-25T20:07:48.236010Z","shell.execute_reply.started":"2023-08-25T20:07:48.235716Z","shell.execute_reply":"2023-08-25T20:07:48.235743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"79-P_L5wZhLY"},"execution_count":null,"outputs":[]}]}