{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nimport torchvision.models as model\nfrom torchsummary import summary\nfrom sklearn.metrics import precision_score, f1_score,recall_score,accuracy_score\nfrom torch.optim.lr_scheduler import StepLR","metadata":{"id":"PqL9L0eSTdax","execution":{"iopub.status.busy":"2023-08-25T17:48:20.806564Z","iopub.execute_input":"2023-08-25T17:48:20.806914Z","iopub.status.idle":"2023-08-25T17:48:20.814072Z","shell.execute_reply.started":"2023-08-25T17:48:20.806883Z","shell.execute_reply":"2023-08-25T17:48:20.812090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir output","metadata":{"execution":{"iopub.status.busy":"2023-08-25T18:05:38.921063Z","iopub.execute_input":"2023-08-25T18:05:38.922268Z","iopub.status.idle":"2023-08-25T18:05:40.013166Z","shell.execute_reply.started":"2023-08-25T18:05:38.922230Z","shell.execute_reply":"2023-08-25T18:05:40.011310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n  def __init__(self,in_channels,out_channels):\n    super(DoubleConv,self).__init__()\n\n    self.conv = nn.Sequential(\n        nn.Conv2d(in_channels,out_channels,3,1,1,bias = False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace = True),\n\n        nn.Conv2d(out_channels,out_channels,3,1,1,bias = False),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace = True),\n    )\n\n  def forward(self,x):\n    return self.conv(x)\n","metadata":{"id":"eVMmroHrTiWH","execution":{"iopub.status.busy":"2023-08-25T17:48:27.718125Z","iopub.execute_input":"2023-08-25T17:48:27.718520Z","iopub.status.idle":"2023-08-25T17:48:27.726746Z","shell.execute_reply.started":"2023-08-25T17:48:27.718487Z","shell.execute_reply":"2023-08-25T17:48:27.725411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNET(nn.Module):\n  def __init__(self,in_channels = 3,out_channels = 1,features = [64,128,256,512]):\n    super(UNET,self).__init__()\n    self.downs = nn.ModuleList()\n    self.ups = nn.ModuleList()\n    self.pool = nn.MaxPool2d(kernel_size = 2,stride = 2)\n\n    # Down part of UNET\n    for feature in features:\n      self.downs.append(DoubleConv(in_channels,feature))\n      in_channels = feature\n\n    # Up part of UNET\n    for feature in reversed(features):\n       self.ups.append(nn.ConvTranspose2d(feature*2,feature,kernel_size = 2,stride = 2))\n       self.ups.append(DoubleConv(feature*2,feature))\n\n    self.bottleneck = DoubleConv(features[-1],features[-1]*2)\n    self.final_conv = nn.Conv2d(features[0],out_channels,kernel_size = 1)\n\n\n  def forward(self,x):\n    skip_connections = []\n\n    for down in self.downs:\n      x = down(x)\n      skip_connections.append(x)\n      x = self.pool(x)\n\n    x = self.bottleneck(x)\n\n    skip_connections = skip_connections[::-1]\n\n    for idx in range(0,len(self.ups),2):\n      x = self.ups[idx](x)\n\n      skip_connection = skip_connections[idx//2]\n\n      if x.shape != skip_connection.shape:\n        x = TF.resize(x,size = skip_connection.shape[2:])\n\n      concat_skip = torch.cat((skip_connection,x),dim = 1)\n      x = self.ups[idx+1](concat_skip)\n\n    return self.final_conv(x)\n\ndef test():\n  x = torch.randn((3,3,417,417))\n  model = UNET(in_channels=3,out_channels = 1)\n  preds = model(x)\n  print('------------------------')\n  print(x.shape)\n  print(preds.shape)\n  # assert preds.shape == x.shape\n\nif __name__ == \"__main__\":\n  test()","metadata":{"id":"qEiQdVDdToH9","execution":{"iopub.status.busy":"2023-08-25T17:48:35.781596Z","iopub.execute_input":"2023-08-25T17:48:35.781955Z","iopub.status.idle":"2023-08-25T17:48:49.663517Z","shell.execute_reply.started":"2023-08-25T17:48:35.781923Z","shell.execute_reply":"2023-08-25T17:48:49.662251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random","metadata":{"id":"MtfgzUCa7xVC","execution":{"iopub.status.busy":"2023-08-25T17:49:01.879216Z","iopub.execute_input":"2023-08-25T17:49:01.879683Z","iopub.status.idle":"2023-08-25T17:49:01.889777Z","shell.execute_reply.started":"2023-08-25T17:49:01.879640Z","shell.execute_reply":"2023-08-25T17:49:01.884823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SlumDataset(Dataset):\n  def __init__(self,image_dir,mask_dir,transform = None):\n    self.image_dir = image_dir\n    self.mask_dir = mask_dir\n    self.transform = transform\n    self.images = os.listdir(image_dir)\n    self.masks = os.listdir(mask_dir)\n\n  def __len__(self):\n    return len(self.masks)\n\n  def __getitem__(self,index):\n    img_path = os.path.join(self.image_dir,self.images[index])\n    mask_path = os.path.join(self.mask_dir,self.masks[index])\n\n    image = np.array(Image.open(img_path))\n    mask = np.array(Image.open(mask_path), dtype = np.float32)\n    mask = (mask//246.0)\n    image = (image/255.0)\n\n\n    if self.transform != None:\n      augment = self.transform(image = image,mask = mask)\n      image = augment['image']\n      mask = augment['mask']\n\n      if(image.shape[1:] != mask.shape):\n        print(\"-------------- ---------------------------\")\n        print(\"!!!Warning!!!\")\n        print(image.shape[1:])\n        print(mask.shape)\n\n    return image,mask","metadata":{"id":"lRG82yGz_TWd","execution":{"iopub.status.busy":"2023-08-25T17:49:06.497931Z","iopub.execute_input":"2023-08-25T17:49:06.498382Z","iopub.status.idle":"2023-08-25T17:49:06.511188Z","shell.execute_reply.started":"2023-08-25T17:49:06.498347Z","shell.execute_reply":"2023-08-25T17:49:06.509871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distributeData(train_image,train_mask,train_val):\n    images = os.listdir(train_image)\n    masks = os.listdir(train_mask)\n\n    for i in range(0,len(images)):\n      images[i] = os.path.join(train_image,images[i])\n\n    for i in range(0,len(masks)):\n      masks[i] = os.path.join(train_mask,masks[i])\n\n\n    train_img, valid_img = images[:int(train_val*len(images))], images[int(train_val*len(images)):]\n    train_mask,valid_mask = masks[:int(train_val*len(masks))],masks[int(train_val*len(masks)):]\n\n    return train_img,train_mask,valid_img,valid_mask","metadata":{"id":"LQLeOKERALL6","execution":{"iopub.status.busy":"2023-08-25T17:49:14.498537Z","iopub.execute_input":"2023-08-25T17:49:14.498928Z","iopub.status.idle":"2023-08-25T17:49:14.507578Z","shell.execute_reply.started":"2023-08-25T17:49:14.498896Z","shell.execute_reply":"2023-08-25T17:49:14.506577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just for checking the above code","metadata":{"id":"BRZdysX-mh2U"}},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torch.utils.data import DataLoader","metadata":{"id":"1hlYs3mGTxq5","execution":{"iopub.status.busy":"2023-08-25T17:49:25.361843Z","iopub.execute_input":"2023-08-25T17:49:25.362658Z","iopub.status.idle":"2023-08-25T17:49:25.367803Z","shell.execute_reply.started":"2023-08-25T17:49:25.362620Z","shell.execute_reply":"2023-08-25T17:49:25.366569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(state,filename = \"my_checkpoint.pth.tar\"):\n  print(\"=> Saving checkpoint\")\n  torch.save(state,filename)","metadata":{"id":"j696D4ZhcMsf","execution":{"iopub.status.busy":"2023-08-25T17:49:32.288495Z","iopub.execute_input":"2023-08-25T17:49:32.289554Z","iopub.status.idle":"2023-08-25T17:49:32.294649Z","shell.execute_reply.started":"2023-08-25T17:49:32.289511Z","shell.execute_reply":"2023-08-25T17:49:32.293718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_checkpoint(checkpoint,model,optimizer):\n  print(\"=> Loading checkpoint\")\n  model.load_state_dict(checkpoint[\"state_dict\"])\n  optimizer.load_state_dict(checkpoint[\"optimizer\"])","metadata":{"id":"ZLI8gkIMcPrh","execution":{"iopub.status.busy":"2023-08-25T17:49:36.485576Z","iopub.execute_input":"2023-08-25T17:49:36.485934Z","iopub.status.idle":"2023-08-25T17:49:36.491210Z","shell.execute_reply.started":"2023-08-25T17:49:36.485904Z","shell.execute_reply":"2023-08-25T17:49:36.490081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_loaders(\n    train_dir,\n    train_mask_dir,\n    val_dir,\n    val_mask_dir,\n    batch_size,\n    train_transform,\n    val_transform,\n    train_val,\n    num_workers = 2,\n    pin_memory = True,\n):\n\n  train_ds = SlumDataset(\n      image_dir = train_dir,\n      mask_dir = train_mask_dir,\n      transform = train_transform\n  )\n\n  train_loader = DataLoader(\n      train_ds,\n      batch_size = batch_size,\n      num_workers = num_workers,\n      pin_memory = pin_memory,\n      shuffle = True,\n  )\n\n  val_ds = SlumDataset(\n      image_dir = val_dir,\n      mask_dir = val_mask_dir,\n      transform = val_transform\n  )\n\n  val_loader = DataLoader(\n      val_ds,\n      batch_size = batch_size,\n      num_workers = num_workers,\n      pin_memory = pin_memory,\n      shuffle = False,\n  )\n\n  return train_loader, val_loader","metadata":{"id":"fVZD13z4_ZI8","execution":{"iopub.status.busy":"2023-08-25T17:49:40.591213Z","iopub.execute_input":"2023-08-25T17:49:40.591641Z","iopub.status.idle":"2023-08-25T17:49:40.600266Z","shell.execute_reply.started":"2023-08-25T17:49:40.591607Z","shell.execute_reply":"2023-08-25T17:49:40.599242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_coefficient(predicted_mask, ground_truth_mask):\n  intersection = np.sum(predicted_mask * ground_truth_mask)\n  predicted_area = np.sum(predicted_mask)\n  ground_truth_area = np.sum(ground_truth_mask)\n\n  dice = (2.0 * intersection) / (predicted_area + ground_truth_area)\n  return dice\n\ndef check_accuracy(loss_fn,loader,model,accuracy_vals,dice_scores,iou_scores,precision_scores,recall_scores,f1_val,val_loss,device = \"cuda\"):\n  num_correct = 0\n  num_pixels = 0\n  dice_s = []\n  precision_s = []\n  f1_s = []\n  iou_s = []\n  recall_s = []\n  accuracy_s = []\n  model.eval()\n\n  with torch.no_grad():\n    for x,y in loader:\n      x = x.to(device)\n      y = y.to(device).unsqueeze(1)\n      pred = torch.sigmoid(model(x))\n\n      val_loss += loss_fn(y,pred).item()\n\n      pred = (pred > 0.5).float()\n      num_correct += (pred == y).sum()\n      num_pixels += torch.numel(pred)\n      preds = pred.cpu().numpy()\n      yy = y.cpu().numpy()\n\n\n      for pred_mask, gt_mask in zip(preds, yy):\n        # Flatten the binary masks for precision calculation\n        pred_mask_flat = pred_mask.flatten()\n        gt_mask_flat = gt_mask.flatten()\n\n        # Calculate recall score for the current mask\n        recall = recall_score(gt_mask_flat, pred_mask_flat)\n        recall_s.append(recall)\n\n        # Calculate precision for the current mask\n        precision = precision_score(gt_mask_flat, pred_mask_flat)\n        precision_s.append(precision)\n\n        # Calculate F1 score for the current mask\n        # F1-score is the harmonic mean of Precision and Recall,\n        # F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n\n        f1 = f1_score(gt_mask_flat, pred_mask_flat)\n        f1_s.append(f1)\n\n        # Calculate the intersection and union of the binary masks\n        intersection = np.sum(pred_mask * gt_mask)\n        union = np.sum(np.logical_or(pred_mask, gt_mask))\n\n        # Calculate IoU for the current mask\n        iou = intersection / union\n        iou_s.append(iou)\n\n        # Calculate Accuarcy for the current mask\n        accuracy = np.mean(gt_mask_flat == pred_mask_flat)\n        accuracy_s.append(accuracy)\n\n        dice_score = dice_coefficient(pred_mask, gt_mask)\n        dice_s.append(dice_score)\n\n  val_loss /= len(loader)\n  accu = (num_correct/num_pixels)*100\n  print(f\"Got {num_correct}/{num_pixels} with accuracy {accu}\")\n\n  # Calculate the average precision over the validation dataset\n  average_precision = np.mean(precision_s)\n  print(f\"Average Precision: {average_precision:.4f}\")\n\n  #Calculate the average accuracy over the validation dataset\n  accuracy_vals.append(np.mean(accuracy_s))\n  print(f'Accuracy: {np.mean(accuracy_s)}')\n\n  # Calculate the average Dice score over the validation dataset\n  average_dice = np.mean(dice_s)\n  print(f\"Average Dice Score: {average_dice:.4f}\")\n\n  # Calculate the average F1 score over the validation dataset\n  average_f1 = np.mean(f1_s)\n  print(f\"Average F1 Score: {average_f1:.4f}\")\n\n  # Calculate the average IoU score over the validation dataset\n  average_iou = np.mean(iou_s)\n  print(f\"Average IoU Score: {average_iou:.4f}\")\n\n  # Calculate average metrics for the epoch\n  iou_scores.append(average_iou)\n  precision_scores.append(average_precision)\n  recall_scores.append(np.mean(recall_s))\n  f1_val.append(average_f1)\n  dice_scores.append(average_dice)\n\n  model.train()","metadata":{"id":"xhDv4-39_gI7","execution":{"iopub.status.busy":"2023-08-25T17:49:43.339960Z","iopub.execute_input":"2023-08-25T17:49:43.340387Z","iopub.status.idle":"2023-08-25T17:49:43.359158Z","shell.execute_reply.started":"2023-08-25T17:49:43.340351Z","shell.execute_reply":"2023-08-25T17:49:43.357812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_predictions_as_imgs(loader,model,folder=\"saved_images/\",device = \"cuda\"):\n  model.eval()\n  for idx, (x,y) in enumerate(loader):\n    x = x.to(device = device)\n\n    with torch.no_grad():\n      preds = torch.sigmoid(model(x))\n      preds = (preds > 0.5).float()\n      # preds = (preds > 0.5).astype(torch.int)\n\n    torchvision.utils.save_image(preds,f\"{folder}/pred_{idx}.jpg\")\n    torchvision.utils.save_image(y.unsqueeze(1),f\"{folder}/{idx}.jpg\")\n\n  model.train()\n\n\ndef my_plot(epochs, loss):\n  plt.xlabel(\"losses\")\n  plt.ylabel(\"Num of epochs\")\n  plt.plot(epochs, loss)","metadata":{"id":"TED9OGwvWACX","execution":{"iopub.status.busy":"2023-08-25T17:49:45.091924Z","iopub.execute_input":"2023-08-25T17:49:45.092363Z","iopub.status.idle":"2023-08-25T17:49:45.101150Z","shell.execute_reply.started":"2023-08-25T17:49:45.092323Z","shell.execute_reply":"2023-08-25T17:49:45.100032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nfrom numpy import random\n\nimport numpy as np\nfrom mpl_toolkits import mplot3d\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-poster')\n\n# Hyperparameters\nLEARNING_RATE = 1e-4\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\nNUM_EPOCHS = 50\nNUM_WORKERS = 2\ntrain_val = 0.8\nIMAGE_HEIGHT = 256\nIMAGE_WIDTH = 256\ntrain_valid = 0.8\n\nPIN_MEMORY = True\nLOAD_MODEL = True\n\nTRAIN_IMG_DIR = '/kaggle/input/slum-dataset/train_img/train_img'\nTRAIN_MASK_DIR = '/kaggle/input/slum-dataset/train_mask-20230822T130205Z-001/train_mask'\nval_dir = '/kaggle/input/slum-dataset/val_img-20230822T130211Z-001/val_img'\nval_mask_dir = '/kaggle/input/slum-dataset/val_mask-20230822T130216Z-001/val_mask'\nsaved_folder = '/kaggle/working/output'\n\n# TRAIN_IMG_DIR = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/Updated_Annotated_data/1 btp/1/images_/train_images'\n# TRAIN_MASK_DIR = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/Updated_Annotated_data/1 btp/1/images_/train_masks'\n# val_dir = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/new_complete_images/val_img'\n# val_mask_dir = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/Updated_Annotated_data/1 btp/1/images_/valid_masks'\n# saved_folder = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/Updated_Annotated_data/1 btp/1/images_/saved_folder'","metadata":{"id":"-yIHRo3_sNlp","outputId":"6cc31027-ccd0-47b0-c948-78fdff15d743","execution":{"iopub.status.busy":"2023-08-25T18:06:09.981364Z","iopub.execute_input":"2023-08-25T18:06:09.981796Z","iopub.status.idle":"2023-08-25T18:06:09.996863Z","shell.execute_reply.started":"2023-08-25T18:06:09.981759Z","shell.execute_reply":"2023-08-25T18:06:09.995598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(epoch,num_epochs,loader,model,optimizer,loss_fn,scaler,iou_train,precision_train,recall_train,f1_train,loss_train,accuracy_train):\n  total_loss = 0.0\n  total_iou = 0.0\n  total_precision = 0.0\n  total_recall = 0.0\n  total_f1 = 0.0\n  total_accuracy = 0.0\n\n  length = len(loader);\n  loop = tqdm(loader)\n\n  for batch_idx, (data,targets) in enumerate(loop):\n    data = data.to(device = device)\n    targets = targets.float().unsqueeze(1).to(device = device)\n    targ = targets\n\n    # forward\n    with torch.cuda.amp.autocast():\n      predictions = torch.sigmoid(model(data))\n\n      loss = loss_fn(predictions,targets)\n      total_loss += loss.item()\n\n      # convert model outputs to binary mask using sigmoid and threshold\n      predicted_masks = predictions\n      predicted_masks = (predicted_masks > 0.5).float()\n\n      preds = predicted_masks.cpu().numpy()\n      yy = targ.cpu().numpy()\n\n      # Calculate the intersection and union of the binary masks\n      intersection = np.sum(preds * yy)\n      union = np.sum(np.logical_or(preds, yy))\n      iou = intersection / union\n\n      precision = precision_score(yy.flatten(), preds.flatten())\n      recall = recall_score(yy.flatten(), preds.flatten())\n      f1 = f1_score(yy.flatten(), preds.flatten())\n      accuracy = np.mean(yy.flatten() == preds.flatten())\n\n      total_iou += iou\n      total_precision += precision\n      total_recall += recall\n      total_f1 += f1\n      total_accuracy += accuracy\n\n    # backward\n    optimizer.zero_grad()\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n    # update tqdm loop\n    loop.set_postfix(loss = loss.item())\n\n  average_loss = total_loss / length\n  average_iou = total_iou / length\n  average_precision = total_precision / length\n  average_recall = total_recall / length\n  average_f1 = total_f1 / length\n  average_accuracy = total_accuracy /length\n\n  # Append metrics and losses to lists for plotting\n  loss_train.append(average_loss)\n  iou_train.append(average_iou)\n  precision_train.append(average_precision)\n  recall_train.append(average_recall)\n  f1_train.append(average_f1)\n  accuracy_train.append(average_accuracy)\n\n  print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n          f\"Loss: {average_loss:.4f}, \"\n          f\"IoU: {average_iou:.4f}, \"\n          f\"Precision: {average_precision:.4f}, \"\n          f\"Recall: {average_recall:.4f}, \"\n          f\"F1: {average_f1:.4f},\"\n          f\"Accuracy: {average_accuracy:4f}\"\n  )\n","metadata":{"id":"oacrLG8WsSbX","execution":{"iopub.status.busy":"2023-08-28T08:58:24.039300Z","iopub.execute_input":"2023-08-28T08:58:24.039651Z","iopub.status.idle":"2023-08-28T08:58:24.061047Z","shell.execute_reply.started":"2023-08-28T08:58:24.039622Z","shell.execute_reply":"2023-08-28T08:58:24.059543Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def func(NUM_EPOCHS,loss_train,iou_train,precision_train,recall_train,f1_train,iou_scores,precision_scores,recall_scores, f1_val,dice_scores, accuracy_vals):\n  epochs = np.arange(1, NUM_EPOCHS + 1)\n  plt.figure(figsize=(12, 8))\n\n  plt.plot(epochs, loss_train, label='Train_Loss')\n  plt.plot(epochs, iou_train, label='Train_IoU')\n  plt.plot(epochs, precision_train, label='Train_Precision')\n  plt.plot(epochs, recall_train, label='Train_Recall')\n  plt.plot(epochs, f1_train, label='Train_F1')\n  # plt.plot(epochs, accuracy_train, label='Train_Accuracy')\n\n  plt.xlabel('Train_Epoch')\n  plt.ylabel('Score / Loss')\n  plt.title('Training Metrics and Losses Over Epochs')\n  plt.legend()\n  plt.grid()\n\n  plt.show()\n\n  # PLot validation metrics\n  plt.figure(figsize=(12, 8))\n\n  plt.plot(epochs, iou_scores, label='validation_IoU')\n  plt.plot(epochs, precision_scores, label='validation_Precision')\n  plt.plot(epochs, recall_scores, label='validation_Recall')\n  plt.plot(epochs, f1_val, label='validation_F1')\n  plt.plot(epochs, dice_scores, label='validation_Dice')\n  plt.plot(epochs,accuracy_vals,label = \"validation_Accuracy\")\n\n  plt.xlabel('validation_Epoch')\n  plt.ylabel('Score')\n  plt.title('Validation Metrics Over Epochs')\n  plt.legend()\n  plt.grid()\n\n  plt.show()\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n  train_transform = A.Compose(\n    [\n      A.Resize(height = IMAGE_HEIGHT,width = IMAGE_WIDTH),\n      A.RandomCrop(height = IMAGE_HEIGHT,width=IMAGE_WIDTH),\n      A.Rotate(limit = 35,p=1.0),\n      A.HorizontalFlip(p=0.5),\n      A.VerticalFlip(p=0.1),\n      A.Normalize(\n          mean=[0.0,0.0,0.0],\n          std = [1.0,1.0,1.0],\n          max_pixel_value = 1.0\n      ),\n      ToTensorV2(),\n    ], is_check_shapes=False\n  )\n\n  val_transform = A.Compose(\n    [\n     A.Resize(height = IMAGE_HEIGHT,width = IMAGE_WIDTH),\n     A.RandomCrop(height = IMAGE_HEIGHT,width=IMAGE_WIDTH),\n     A.Normalize(\n      mean=[0.0,0.0,0.0],\n      std = [1.0,1.0,1.0],\n      max_pixel_value = 1.0\n    ),\n    ToTensorV2(),\n  ], is_check_shapes=False\n  )\n\n  train_loader,val_loader = get_loaders(\n      TRAIN_IMG_DIR,\n      TRAIN_MASK_DIR,\n      val_dir,\n      val_mask_dir,\n      BATCH_SIZE,\n      train_transform,\n      val_transform,\n      train_val,\n      NUM_WORKERS,\n      PIN_MEMORY,\n  )\n\n  model = UNET().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  loss_fn = nn.BCEWithLogitsLoss()\n\n  optimizer = optim.Adam(model.parameters(),lr = LEARNING_RATE)\n  scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n\n  # if LOAD_MODEL:\n    # load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"),model)\n    # load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"),model,optimizer=optimizer)\n\n  scaler = torch.cuda.amp.GradScaler()\n\n  # Initialize lists to store metric values for each epoch for validation dataset\n  iou_scores = []\n  precision_scores = []\n  recall_scores = []\n  f1_val = []\n  dice_scores = []\n  accuracy_vals = []\n\n  # Initialize lists to store metric values for each epoch for training dataset\n  iou_train = []\n  precision_train = []\n  recall_train = []\n  f1_train = []\n  loss_train = []\n  accuracy_train = []\n\n  # Number of patience for early stopping\n  patience = 10\n  # best_val_loss = float('inf')\n  best_val_loss = torch.tensor(float('inf'))\n  counter = 0\n\n  for epoch in range(NUM_EPOCHS):\n    scheduler.step()\n    train_fn(epoch,NUM_EPOCHS,train_loader,model,optimizer,loss_fn,scaler,iou_train,precision_train,recall_train,f1_train,loss_train,accuracy_train)\n\n    # save model\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    save_checkpoint(checkpoint)\n\n    # check_accuracy\n    val_loss = 0.0\n    check_accuracy(loss_fn,val_loader,model,accuracy_vals,dice_scores,iou_scores,precision_scores,recall_scores,f1_val,val_loss,device = DEVICE)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered.\")\n            save_predictions_as_imgs(\n            val_loader,model,folder=saved_folder,device = DEVICE\n            )\n            func(epoch,loss_train,iou_train,precision_train,recall_train,f1_train,iou_scores,precision_scores,recall_scores, f1_val,dice_scores, accuracy_vals)\n            break\n\n    # print some examples to the folder\n    save_predictions_as_imgs(\n        val_loader,model,folder=saved_folder,device = DEVICE\n    )\n  print(\"Training finished\")\n\n  # Plot metrics and losses\n  # Plot for Training dataset\n  func(NUM_EPOCHS,loss_train,iou_train,precision_train,recall_train,f1_train,iou_scores,precision_scores,recall_scores, f1_val,dice_scores, accuracy_vals)\n\n\n  summary(model,input_size = (3,IMAGE_HEIGHT,IMAGE_WIDTH))\n\nif __name__ == \"__main__\":\n  main()","metadata":{"id":"NX7hZfvJsdtV","execution":{"iopub.status.busy":"2023-08-25T18:06:19.633576Z","iopub.execute_input":"2023-08-25T18:06:19.633997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}