{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms.functional as TF\nfrom sklearn.metrics import precision_score, f1_score,recall_score,accuracy_score\n!pip install torchsummary\nfrom torchsummary import summary\nfrom torch.optim.lr_scheduler import StepLR\n\nimport os\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom torch.utils.data import Dataset,random_split,DataLoader,Subset","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:17.007846Z","iopub.execute_input":"2023-08-27T13:08:17.008473Z","iopub.status.idle":"2023-08-27T13:08:28.513442Z","shell.execute_reply.started":"2023-08-27T13:08:17.008428Z","shell.execute_reply":"2023-08-27T13:08:28.512170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir output","metadata":{"execution":{"iopub.status.busy":"2023-08-27T13:08:28.516128Z","iopub.execute_input":"2023-08-27T13:08:28.516898Z","iopub.status.idle":"2023-08-27T13:08:29.504954Z","shell.execute_reply.started":"2023-08-27T13:08:28.516856Z","shell.execute_reply":"2023-08-27T13:08:29.503751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SlumDataset(Dataset):\n  def __init__(self,image_dir,mask_dir,transform = None):\n    self.image_dir = image_dir\n    self.mask_dir = mask_dir\n    self.transform = transform\n    self.images = os.listdir(image_dir)\n    self.masks = os.listdir(mask_dir)\n\n  def __len__(self):\n    return len(self.masks)\n\n  def __getitem__(self,index):\n    img_path = os.path.join(self.image_dir,self.images[index])\n    mask_path = os.path.join(self.mask_dir,self.masks[index])\n\n    image = np.array(Image.open(img_path))\n    mask = np.array(Image.open(mask_path), dtype = np.float32)\n    mask = (mask//246.0)\n    image = (image/255.0)\n\n\n    if self.transform != None:\n      augment = self.transform(image = image,mask = mask)\n      image = augment['image']\n      mask = augment['mask']\n\n      if(image.shape[1:] != mask.shape):\n        print(\"-------------- ---------------------------\")\n        print(\"!!!Warning!!!\")\n        print(image.shape[1:])\n        print(mask.shape)\n\n    return image,mask","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:29.507468Z","iopub.execute_input":"2023-08-27T13:08:29.507885Z","iopub.status.idle":"2023-08-27T13:08:29.518304Z","shell.execute_reply.started":"2023-08-27T13:08:29.507848Z","shell.execute_reply":"2023-08-27T13:08:29.517334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(state,filename = \"my_checkpoint.pth.tar\"):\n  print(\"=> Saving checkpoint\")\n  torch.save(state,filename)\n\ndef load_checkpoint(checkpoint,model,optimizer):\n  print(\"=> Loading checkpoint\")\n  model.load_state_dict(checkpoint[\"state_dict\"])\n  optimizer.load_state_dict(checkpoint[\"optimizer\"])","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:29.521303Z","iopub.execute_input":"2023-08-27T13:08:29.522178Z","iopub.status.idle":"2023-08-27T13:08:29.538778Z","shell.execute_reply.started":"2023-08-27T13:08:29.522144Z","shell.execute_reply":"2023-08-27T13:08:29.537697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_loaders(\n    train_dir,\n    train_mask_dir,\n    val_dir,\n    val_mask_dir,\n    batch_size,\n    train_transform,\n    val_transform,\n    train_val,\n    num_workers = 2,\n    pin_memory = True,\n):\n\n  train_ds = SlumDataset(\n      image_dir = train_dir,\n      mask_dir = train_mask_dir,\n      transform = train_transform\n  )\n\n  train_loader = DataLoader(\n      train_ds,\n      batch_size = batch_size,\n      num_workers = num_workers,\n      pin_memory = pin_memory,\n      shuffle = True,\n  )\n\n  val_ds = SlumDataset(\n      image_dir = val_dir,\n      mask_dir = val_mask_dir,\n      transform = val_transform\n  )\n\n  val_loader = DataLoader(\n      val_ds,\n      batch_size = batch_size,\n      num_workers = num_workers,\n      pin_memory = pin_memory,\n      shuffle = False,\n  )\n\n  return train_loader, val_loader","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:29.540575Z","iopub.execute_input":"2023-08-27T13:08:29.541293Z","iopub.status.idle":"2023-08-27T13:08:29.550808Z","shell.execute_reply.started":"2023-08-27T13:08:29.541255Z","shell.execute_reply":"2023-08-27T13:08:29.549835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_coefficient(predicted_mask, ground_truth_mask):\n  intersection = np.sum(predicted_mask * ground_truth_mask)\n  predicted_area = np.sum(predicted_mask)\n  ground_truth_area = np.sum(ground_truth_mask)\n\n  dice = (2.0 * intersection) / (predicted_area + ground_truth_area)\n  return dice\n\ndef check_accuracy(loss_fn,loader,model,accuracy_vals,dice_scores,iou_scores,precision_scores,recall_scores,f1_val,val_loss,device = \"cuda\"):\n  num_correct = 0\n  num_pixels = 0\n  dice_s = []\n  precision_s = []\n  f1_s = []\n  iou_s = []\n  recall_s = []\n  accuracy_s = []\n  model.eval()\n\n  with torch.no_grad():\n    for x,y in loader:\n      x = x.to(device)\n      y = y.to(device).unsqueeze(1)\n      pred = torch.sigmoid(model(x))\n\n      val_loss += loss_fn(y,pred).item()\n\n      pred = (pred > 0.5).float()\n      num_correct += (pred == y).sum()\n      num_pixels += torch.numel(pred)\n      preds = pred.cpu().numpy()\n      yy = y.cpu().numpy()\n\n\n      for pred_mask, gt_mask in zip(preds, yy):\n        # Flatten the binary masks for precision calculation\n        pred_mask_flat = pred_mask.flatten()\n        gt_mask_flat = gt_mask.flatten()\n\n        # Calculate recall score for the current mask\n        recall = recall_score(gt_mask_flat, pred_mask_flat)\n        recall_s.append(recall)\n\n        # Calculate precision for the current mask\n        precision = precision_score(gt_mask_flat, pred_mask_flat)\n        precision_s.append(precision)\n\n        # Calculate F1 score for the current mask\n        # F1-score is the harmonic mean of Precision and Recall,\n        # F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n\n        f1 = f1_score(gt_mask_flat, pred_mask_flat)\n        f1_s.append(f1)\n\n        # Calculate the intersection and union of the binary masks\n        intersection = np.sum(pred_mask * gt_mask)\n        union = np.sum(np.logical_or(pred_mask, gt_mask))\n\n        # Calculate IoU for the current mask\n        iou = intersection / union\n        iou_s.append(iou)\n\n        # Calculate Accuarcy for the current mask\n        accuracy = np.mean(gt_mask_flat == pred_mask_flat)\n        accuracy_s.append(accuracy)\n\n        dice_score = dice_coefficient(pred_mask, gt_mask)\n        dice_s.append(dice_score)\n\n  val_loss /= len(loader)\n  accu = (num_correct/num_pixels)*100\n  print(f\"Got {num_correct}/{num_pixels} with accuracy {accu}\")\n\n  # Calculate the average precision over the validation dataset\n  average_precision = np.mean(precision_s)\n  print(f\"Average Precision: {average_precision:.4f}\")\n\n  #Calculate the average accuracy over the validation dataset\n  accuracy_vals.append(np.mean(accuracy_s))\n  print(f'Accuracy: {np.mean(accuracy_s)}')\n\n  # Calculate the average Dice score over the validation dataset\n  average_dice = np.mean(dice_s)\n  print(f\"Average Dice Score: {average_dice:.4f}\")\n\n  # Calculate the average F1 score over the validation dataset\n  average_f1 = np.mean(f1_s)\n  print(f\"Average F1 Score: {average_f1:.4f}\")\n\n  # Calculate the average IoU score over the validation dataset\n  average_iou = np.mean(iou_s)\n  print(f\"Average IoU Score: {average_iou:.4f}\")\n\n  # Calculate average metrics for the epoch\n  iou_scores.append(average_iou)\n  precision_scores.append(average_precision)\n  recall_scores.append(np.mean(recall_s))\n  f1_val.append(average_f1)\n  dice_scores.append(average_dice)\n\n  model.train()","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:29.553400Z","iopub.execute_input":"2023-08-27T13:08:29.554172Z","iopub.status.idle":"2023-08-27T13:08:29.571603Z","shell.execute_reply.started":"2023-08-27T13:08:29.554139Z","shell.execute_reply":"2023-08-27T13:08:29.570463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_predictions_as_imgs(loader,model,folder=\"saved_images/\",device = \"cuda\"):\n  model.eval()\n  for idx, (x,y) in enumerate(loader):\n    x = x.to(device = device)\n    y = y.unsqueeze(1).to(device)\n\n    with torch.no_grad():\n      preds = torch.sigmoid(model(x))\n      preds = (preds > 0.5).float()\n\n    torchvision.utils.save_image(preds,f\"{folder}/pred_{idx}.jpg\")\n    torchvision.utils.save_image(y,f\"{folder}/{idx}.jpg\")\n\n  model.train()","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:29.573437Z","iopub.execute_input":"2023-08-27T13:08:29.573872Z","iopub.status.idle":"2023-08-27T13:08:29.586081Z","shell.execute_reply.started":"2023-08-27T13:08:29.573838Z","shell.execute_reply":"2023-08-27T13:08:29.585075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n  def __init__(self,in_channels,out_channels,padding=1,kernel_size = 3,stride = 1,with_nonlinearity=True):\n    super().__init__()\n    self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride)\n    self.bn = nn.BatchNorm2d(out_channels)\n    self.relu = nn.ReLU()\n    self.with_nonlinearity = with_nonlinearity\n\n  def forward(self,x):\n    x = self.conv(x)\n    x = self.bn(x)\n    if self.with_nonlinearity:\n        x = self.relu(x)\n\n    return x","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:29.587845Z","iopub.execute_input":"2023-08-27T13:08:29.588195Z","iopub.status.idle":"2023-08-27T13:08:29.597334Z","shell.execute_reply.started":"2023-08-27T13:08:29.588163Z","shell.execute_reply":"2023-08-27T13:08:29.596350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Bridge(nn.Module):\n  def __init__(self, in_channels, out_channels):\n    super().__init__()\n    self.bridge = nn.Sequential(\n        ConvBlock(in_channels, out_channels),\n        ConvBlock(out_channels, out_channels)\n    )\n\n  def forward(self, x):\n    x = self.bridge(x)\n    return x","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:29.598909Z","iopub.execute_input":"2023-08-27T13:08:29.599428Z","iopub.status.idle":"2023-08-27T13:08:29.610807Z","shell.execute_reply.started":"2023-08-27T13:08:29.599395Z","shell.execute_reply":"2023-08-27T13:08:29.609827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UpBlockForUNetWithResNet50(nn.Module):\n  \"\"\"\n    Up block that encapsulates one up-sampling step which consists of Upsample -> ConvBlock -> ConvBlock\n  \"\"\"\n\n  def __init__(self, in_channels, out_channels, up_conv_in_channels=None, up_conv_out_channels=None,upsampling_method=\"conv_transpose\"):\n    super().__init__()\n    if up_conv_in_channels == None:\n        up_conv_in_channels = in_channels\n\n    if up_conv_out_channels == None:\n        up_conv_out_channels = out_channels\n\n    if upsampling_method == \"conv_transpose\":\n        self.upsample = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2)\n\n    self.conv_block_1 = ConvBlock(in_channels, out_channels)\n    self.conv_block_2 = ConvBlock(out_channels, out_channels)\n\n  def forward(self, up_x, down_x):\n    \"\"\"\n      :param up_x: this is the output from the previous up block\n      :param down_x: this is the output from the down block\n      :return: upsampled feature map\n    \"\"\"\n\n    x = self.upsample(up_x)\n\n    if x.shape != down_x.shape:\n      x = TF.resize(x,size = down_x.shape[2:],antialias=False)\n\n    x = torch.cat([x, down_x], 1)\n    x = self.conv_block_1(x)\n    x = self.conv_block_2(x)\n    return x","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:29.614895Z","iopub.execute_input":"2023-08-27T13:08:29.615169Z","iopub.status.idle":"2023-08-27T13:08:29.625172Z","shell.execute_reply.started":"2023-08-27T13:08:29.615146Z","shell.execute_reply":"2023-08-27T13:08:29.623583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNetWithResnet50Encoder(nn.Module):\n  DEPTH = 6\n\n  def __init__(self, n_classes=1):\n    super().__init__()\n    resnet = torchvision.models.resnet.resnet50(pretrained=True)\n    for param in resnet.parameters():\n      param.requires_grad = False\n\n    down_blocks = []\n    up_blocks = []\n\n    self.input_block = nn.Sequential(*list(resnet.children()))[:3]\n    self.input_pool = list(resnet.children())[3]\n\n    for bottleneck in list(resnet.children()):\n        if isinstance(bottleneck, nn.Sequential):\n            down_blocks.append(bottleneck)\n\n    self.down_blocks = nn.ModuleList(down_blocks)\n    self.bridge = Bridge(2048, 2048)\n\n    up_blocks.append(UpBlockForUNetWithResNet50(2048, 1024))\n    up_blocks.append(UpBlockForUNetWithResNet50(1024, 512))\n    up_blocks.append(UpBlockForUNetWithResNet50(512, 256))\n    up_blocks.append(UpBlockForUNetWithResNet50(in_channels=128 + 64, out_channels=128,up_conv_in_channels=256, up_conv_out_channels=128))\n    up_blocks.append(UpBlockForUNetWithResNet50(in_channels=64 + 3, out_channels=64,up_conv_in_channels=128, up_conv_out_channels=64))\n\n    self.up_blocks = nn.ModuleList(up_blocks)\n    self.out = nn.Conv2d(64, n_classes, kernel_size=1, stride=1)\n\n  def forward(self, x, with_output_feature_map=False):\n    pre_pools = dict()\n    pre_pools[f\"layer_0\"] = x\n\n    x = self.input_block(x)\n    pre_pools[f\"layer_1\"] = x\n\n    x = self.input_pool(x)\n\n    for i, block in enumerate(self.down_blocks, 2):\n        x = block(x)\n        if i == (UNetWithResnet50Encoder.DEPTH - 1):\n            continue\n        pre_pools[f\"layer_{i}\"] = x\n\n    x = self.bridge(x)\n\n    for i, block in enumerate(self.up_blocks, 1):\n        key = f\"layer_{UNetWithResnet50Encoder.DEPTH - 1 - i}\"\n        x = block(x, pre_pools[key])\n\n    output_feature_map = x\n\n    x = self.out(x)\n    del pre_pools\n\n    if with_output_feature_map:\n        return x, output_feature_map\n    else:\n        return x","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:29.627284Z","iopub.execute_input":"2023-08-27T13:08:29.628154Z","iopub.status.idle":"2023-08-27T13:08:29.642008Z","shell.execute_reply.started":"2023-08-27T13:08:29.628121Z","shell.execute_reply":"2023-08-27T13:08:29.641155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nfrom numpy import random\n\nimport numpy as np\nfrom mpl_toolkits import mplot3d\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-poster')\n\n# Hyperparameters\nLEARNING_RATE = 1e-4\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\nNUM_EPOCHS = 15\nNUM_WORKERS = 2\ntrain_val = 0.8\nIMAGE_HEIGHT = 256\nIMAGE_WIDTH = 256\ntrain_valid = 0.8\n\nPIN_MEMORY = True\nLOAD_MODEL = True\n\nTRAIN_IMG_DIR = '/kaggle/input/slum-dataset/train_img/train_img'\nTRAIN_MASK_DIR = '/kaggle/input/slum-dataset/train_mask-20230822T130205Z-001/train_mask'\nval_dir = '/kaggle/input/slum-dataset/val_img-20230822T130211Z-001/val_img'\nval_mask_dir = '/kaggle/input/slum-dataset/val_mask-20230822T130216Z-001/val_mask'\nsaved_folder = '/kaggle/working/output'","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:29.643519Z","iopub.execute_input":"2023-08-27T13:08:29.644127Z","iopub.status.idle":"2023-08-27T13:08:29.659610Z","shell.execute_reply.started":"2023-08-27T13:08:29.644091Z","shell.execute_reply":"2023-08-27T13:08:29.658595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(epoch,num_epochs,loader,model,optimizer,loss_fn,scaler,iou_train,precision_train,recall_train,f1_train,loss_train,accuracy_train):\n  total_loss = 0.0\n  total_iou = 0.0\n  total_precision = 0.0\n  total_recall = 0.0\n  total_f1 = 0.0\n  total_accuracy = 0.0\n\n  length = len(loader);\n  loop = tqdm(loader)\n\n  for batch_idx, (data,targets) in enumerate(loop):\n    data = data.to(device = device)\n    targets = targets.float().unsqueeze(1).to(device = device)\n    targ = targets\n\n    # forward\n    with torch.cuda.amp.autocast():\n      predictions = torch.sigmoid(model(data))\n\n      loss = loss_fn(predictions,targets)\n      total_loss += loss.item()\n\n      # convert model outputs to binary mask using sigmoid and threshold\n      predicted_masks = predictions\n      predicted_masks = (predicted_masks > 0.5).float()\n\n      preds = predicted_masks.cpu().numpy()\n      yy = targ.cpu().numpy()\n\n      # Calculate the intersection and union of the binary masks\n      intersection = np.sum(preds * yy)\n      union = np.sum(np.logical_or(preds, yy))\n      iou = intersection / union\n\n      precision = precision_score(yy.flatten(), preds.flatten())\n      recall = recall_score(yy.flatten(), preds.flatten())\n      f1 = f1_score(yy.flatten(), preds.flatten())\n      accuracy = np.mean(yy.flatten() == preds.flatten())\n\n      total_iou += iou\n      total_precision += precision\n      total_recall += recall\n      total_f1 += f1\n      total_accuracy += accuracy\n\n    # backward\n    optimizer.zero_grad()\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n    # update tqdm loop\n    loop.set_postfix(loss = loss.item())\n\n  average_loss = total_loss / length\n  average_iou = total_iou / length\n  average_precision = total_precision / length\n  average_recall = total_recall / length\n  average_f1 = total_f1 / length\n  average_accuracy = accuracy /length\n\n  # Append metrics and losses to lists for plotting\n  loss_train.append(average_loss)\n  iou_train.append(average_iou)\n  precision_train.append(average_precision)\n  recall_train.append(average_recall)\n  f1_train.append(average_f1)\n  accuracy_train.append(average_accuracy)\n\n  print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n          f\"Loss: {average_loss:.4f}, \"\n          f\"IoU: {average_iou:.4f}, \"\n          f\"Precision: {average_precision:.4f}, \"\n          f\"Recall: {average_recall:.4f}, \"\n          f\"F1: {average_f1:.4f},\"\n          f\"Accuracy: {average_accuracy:4f}\"\n  )\n","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:29.661508Z","iopub.execute_input":"2023-08-27T13:08:29.661828Z","iopub.status.idle":"2023-08-27T13:08:29.678612Z","shell.execute_reply.started":"2023-08-27T13:08:29.661803Z","shell.execute_reply":"2023-08-27T13:08:29.677583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def results(NUM_EPOCHS,loss_train,iou_train,precision_train,recall_train,f1_train,iou_scores,precision_scores, recall_scores,f1_val,dice_scores,accuracy_vals):\n  epochs = np.arange(1, NUM_EPOCHS + 1) \n  plt.figure(figsize=(12, 8))\n\n  plt.plot(epochs, loss_train, label='Train_Loss')\n  plt.plot(epochs, iou_train, label='Train_IoU')\n  plt.plot(epochs, precision_train, label='Train_Precision')\n  plt.plot(epochs, recall_train, label='Train_Recall')\n  plt.plot(epochs, f1_train, label='Train_F1')\n  # plt.plot(epochs, accuracy_train, label='Train_Accuracy')\n\n  plt.xlabel('Train_Epoch')\n  plt.ylabel('Score / Loss')\n  plt.title('Training Metrics and Losses Over Epochs')\n  plt.legend()\n  plt.grid()\n\n  plt.show()\n\n  # PLot validation metrics\n  plt.figure(figsize=(12, 8))\n\n  plt.plot(epochs, iou_scores, label='validation_IoU')\n  plt.plot(epochs, precision_scores, label='validation_Precision')\n  plt.plot(epochs, recall_scores, label='validation_Recall')\n  plt.plot(epochs, f1_val, label='validation_F1')\n  plt.plot(epochs, dice_scores, label='validation_Dice')\n  plt.plot(epochs,accuracy_vals,label = \"validation_Accuracy\")\n\n  plt.xlabel('validation_Epoch')\n  plt.ylabel('Score')\n  plt.title('Validation Metrics Over Epochs')\n  plt.legend()\n  plt.grid()\n\n  plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-27T13:08:29.681830Z","iopub.execute_input":"2023-08-27T13:08:29.682144Z","iopub.status.idle":"2023-08-27T13:08:29.695143Z","shell.execute_reply.started":"2023-08-27T13:08:29.682098Z","shell.execute_reply":"2023-08-27T13:08:29.694052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n  train_transform = A.Compose(\n    [\n      A.Resize(height = IMAGE_HEIGHT,width = IMAGE_WIDTH),\n      A.RandomCrop(height = IMAGE_HEIGHT,width=IMAGE_WIDTH),\n      A.Rotate(limit = 35,p=1.0),\n      A.HorizontalFlip(p=0.5),\n      A.VerticalFlip(p=0.1),\n      A.Normalize(\n          mean=[0.0,0.0,0.0],\n          std = [1.0,1.0,1.0],\n          max_pixel_value = 1.0\n      ),\n      ToTensorV2(),\n    ], is_check_shapes=False\n  )\n\n  val_transform = A.Compose(\n    [\n     A.Resize(height = IMAGE_HEIGHT,width = IMAGE_WIDTH),\n     A.RandomCrop(height = IMAGE_HEIGHT,width=IMAGE_WIDTH),\n     A.Normalize(\n      mean=[0.0,0.0,0.0],\n      std = [1.0,1.0,1.0],\n      max_pixel_value = 1.0\n    ),\n    ToTensorV2(),\n  ], is_check_shapes=False\n  )\n\n  train_loader,val_loader = get_loaders(\n      TRAIN_IMG_DIR,\n      TRAIN_MASK_DIR,\n      val_dir,\n      val_mask_dir,\n      BATCH_SIZE,\n      train_transform,\n      val_transform,\n      train_val,\n      NUM_WORKERS,\n      PIN_MEMORY,\n  )\n\n  model = UNetWithResnet50Encoder().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  loss_fn = nn.BCEWithLogitsLoss()\n\n  optimizer = optim.Adam(model.parameters(),lr = LEARNING_RATE)\n  scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n\n  # if LOAD_MODEL:\n    # load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"),model)\n    # load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"),model,optimizer=optimizer)\n\n  scaler = torch.cuda.amp.GradScaler()\n\n  # Initialize lists to store metric values for each epoch for validation dataset\n  iou_scores = []\n  precision_scores = []\n  recall_scores = []\n  f1_val = []\n  dice_scores = []\n  accuracy_vals = []\n\n  # Initialize lists to store metric values for each epoch for training dataset\n  iou_train = []\n  precision_train = []\n  recall_train = []\n  f1_train = []\n  loss_train = []\n  accuracy_train = []\n\n  # Number of patience for early stopping\n  patience = 10\n  # best_val_loss = float('inf')\n  best_val_loss = torch.tensor(float('inf'))\n  counter = 0\n\n  for epoch in range(NUM_EPOCHS):\n    scheduler.step()\n    train_fn(epoch,NUM_EPOCHS,train_loader,model,optimizer,loss_fn,scaler,iou_train,precision_train,recall_train,f1_train,loss_train,accuracy_train)\n\n    # save model\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    save_checkpoint(checkpoint)\n\n    # check_accuracy\n    val_loss = 0.0\n    check_accuracy(loss_fn,val_loader,model,accuracy_vals,dice_scores,iou_scores,precision_scores,recall_scores,f1_val,val_loss,device = DEVICE)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered.\")\n            results(NUM_EPOCHS,loss_train,iou_train,precision_train,recall_train,f1_train,iou_scores,precision_scores, recall_scores,f1_val,dice_scores,accuracy_vals)\n            break\n\n    # print some examples to the folder\n    save_predictions_as_imgs(\n        val_loader,model,folder=saved_folder,device = DEVICE\n    )\n  print(\"Training finished\")\n\n  # Plot metrics and losses\n  # Plot for Training dataset\n  results(NUM_EPOCHS,loss_train,iou_train,precision_train,recall_train,f1_train,iou_scores,precision_scores, recall_scores,f1_val,dice_scores,accuracy_vals)\n    \n  summary(model,input_size = (3,IMAGE_HEIGHT,IMAGE_WIDTH))\n\nif __name__ == \"__main__\":\n  main()","metadata":{"_uuid":"3e7d7a7a-c232-42d1-a334-d953d1fd5216","_cell_guid":"c4707a08-6f58-4ecf-b11f-c3568617e9dc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-27T13:08:29.697306Z","iopub.execute_input":"2023-08-27T13:08:29.697653Z","iopub.status.idle":"2023-08-27T13:32:48.473879Z","shell.execute_reply.started":"2023-08-27T13:08:29.697619Z","shell.execute_reply":"2023-08-27T13:32:48.472771Z"},"trusted":true},"execution_count":null,"outputs":[]}]}