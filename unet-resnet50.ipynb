{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk-9ofOeXthG",
        "outputId": "975a1247-4823-4f15-d44c-c01384b008f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u1XzYHKZXP7",
        "outputId": "279e8c3f-3a5c-417e-e177-8c9836a70fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "from sklearn.metrics import precision_score, f1_score,recall_score,accuracy_score\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4fsmVR-wz-_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.utils.data import Dataset,random_split,DataLoader,Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Maq6sNgNUm7N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gi_Xp97w2Tq"
      },
      "outputs": [],
      "source": [
        "class SlumDataset(Dataset):\n",
        "  def __init__(self,image_dir,mask_dir,transform = None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "    self.images = os.listdir(image_dir)\n",
        "    self.masks = os.listdir(mask_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.masks)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    img_path = os.path.join(self.image_dir,self.images[index])\n",
        "    mask_path = os.path.join(self.mask_dir,self.masks[index])\n",
        "\n",
        "    image = np.array(Image.open(img_path))\n",
        "    mask = np.array(Image.open(mask_path), dtype = np.float32)\n",
        "    mask = (mask//246.0)\n",
        "    image = (image/255.0)\n",
        "\n",
        "\n",
        "    if self.transform != None:\n",
        "      augment = self.transform(image = image,mask = mask)\n",
        "      image = augment['image']\n",
        "      mask = augment['mask']\n",
        "\n",
        "      if(image.shape[1:] != mask.shape):\n",
        "        print(\"-------------- ---------------------------\")\n",
        "        print(\"!!!Warning!!!\")\n",
        "        print(image.shape[1:])\n",
        "        print(mask.shape)\n",
        "\n",
        "    return image,mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy5IqgZuw7PH"
      },
      "outputs": [],
      "source": [
        "def distributeData(train_image,train_mask,train_val):\n",
        "    images = os.listdir(train_image)\n",
        "    masks = os.listdir(train_mask)\n",
        "\n",
        "    for i in range(0,len(images)):\n",
        "      images[i] = os.path.join(train_image,images[i])\n",
        "\n",
        "    for i in range(0,len(masks)):\n",
        "      masks[i] = os.path.join(train_mask,masks[i])\n",
        "\n",
        "    train_img, valid_img = images[:int(train_val*len(images))], images[int(train_val*len(images)):]\n",
        "    train_mask,valid_mask = masks[:int(train_val*len(masks))],masks[int(train_val*len(masks)):]\n",
        "\n",
        "    return train_img,train_mask,valid_img,valid_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHvoNqamw_bm"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state,filename = \"my_checkpoint.pth.tar\"):\n",
        "  print(\"=> Saving checkpoint\")\n",
        "  torch.save(state,filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlxMjucoxCDR"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(checkpoint,model,optimizer):\n",
        "  print(\"=> Loading checkpoint\")\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "  optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yuk_bzgBT8fI"
      },
      "outputs": [],
      "source": [
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_mask_dir,\n",
        "    val_dir,\n",
        "    val_mask_dir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    train_val,\n",
        "    num_workers = 2,\n",
        "    pin_memory = True,\n",
        "):\n",
        "\n",
        "  train_ds = SlumDataset(\n",
        "      image_dir = train_dir,\n",
        "      mask_dir = train_mask_dir,\n",
        "      transform = train_transform\n",
        "  )\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "      train_ds,\n",
        "      batch_size = batch_size,\n",
        "      num_workers = num_workers,\n",
        "      pin_memory = pin_memory,\n",
        "      shuffle = True,\n",
        "  )\n",
        "\n",
        "  val_ds = SlumDataset(\n",
        "      image_dir = val_dir,\n",
        "      mask_dir = val_mask_dir,\n",
        "      transform = val_transform\n",
        "  )\n",
        "\n",
        "  val_loader = DataLoader(\n",
        "      val_ds,\n",
        "      batch_size = batch_size,\n",
        "      num_workers = num_workers,\n",
        "      pin_memory = pin_memory,\n",
        "      shuffle = False,\n",
        "  )\n",
        "\n",
        "  return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coefficient(predicted_mask, ground_truth_mask):\n",
        "  intersection = np.sum(predicted_mask * ground_truth_mask)\n",
        "  predicted_area = np.sum(predicted_mask)\n",
        "  ground_truth_area = np.sum(ground_truth_mask)\n",
        "\n",
        "  dice = (2.0 * intersection) / (predicted_area + ground_truth_area)\n",
        "  return dice\n",
        "\n",
        "def check_accuracy(loss_fn,loader,model,accuracy_vals,dice_scores,iou_scores,precision_scores,recall_scores,f1_val,val_loss,device = \"cuda\"):\n",
        "  num_correct = 0\n",
        "  num_pixels = 0\n",
        "  dice_s = []\n",
        "  precision_s = []\n",
        "  f1_s = []\n",
        "  iou_s = []\n",
        "  recall_s = []\n",
        "  accuracy_s = []\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x,y in loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device).unsqueeze(1)\n",
        "      pred = torch.sigmoid(model(x))\n",
        "\n",
        "      val_loss += loss_fn(y,pred).item()\n",
        "\n",
        "      pred = (pred > 0.5).float()\n",
        "      num_correct += (pred == y).sum()\n",
        "      num_pixels += torch.numel(pred)\n",
        "      preds = pred.cpu().numpy()\n",
        "      yy = y.cpu().numpy()\n",
        "\n",
        "\n",
        "      for pred_mask, gt_mask in zip(preds, yy):\n",
        "        # Flatten the binary masks for precision calculation\n",
        "        pred_mask_flat = pred_mask.flatten()\n",
        "        gt_mask_flat = gt_mask.flatten()\n",
        "\n",
        "        # Calculate recall score for the current mask\n",
        "        recall = recall_score(gt_mask_flat, pred_mask_flat)\n",
        "        recall_s.append(recall)\n",
        "\n",
        "        # Calculate precision for the current mask\n",
        "        precision = precision_score(gt_mask_flat, pred_mask_flat)\n",
        "        precision_s.append(precision)\n",
        "\n",
        "        # Calculate F1 score for the current mask\n",
        "        # F1-score is the harmonic mean of Precision and Recall,\n",
        "        # F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "        f1 = f1_score(gt_mask_flat, pred_mask_flat)\n",
        "        f1_s.append(f1)\n",
        "\n",
        "        # Calculate the intersection and union of the binary masks\n",
        "        intersection = np.sum(pred_mask * gt_mask)\n",
        "        union = np.sum(np.logical_or(pred_mask, gt_mask))\n",
        "\n",
        "        # Calculate IoU for the current mask\n",
        "        iou = intersection / union\n",
        "        iou_s.append(iou)\n",
        "\n",
        "        # Calculate Accuarcy for the current mask\n",
        "        accuracy = np.mean(gt_mask_flat == pred_mask_flat)\n",
        "        accuracy_s.append(accuracy)\n",
        "\n",
        "        dice_score = dice_coefficient(pred_mask, gt_mask)\n",
        "        dice_s.append(dice_score)\n",
        "\n",
        "  val_loss /= len(loader)\n",
        "  accu = (num_correct/num_pixels)*100\n",
        "  print(f\"Got {num_correct}/{num_pixels} with accuracy {accu}\")\n",
        "\n",
        "  # Calculate the average precision over the validation dataset\n",
        "  average_precision = np.mean(precision_s)\n",
        "  print(f\"Average Precision: {average_precision:.4f}\")\n",
        "\n",
        "  #Calculate the average accuracy over the validation dataset\n",
        "  accuracy_vals.append(np.mean(accuracy_s))\n",
        "  print(f'Accuracy: {np.mean(accuracy_s)}')\n",
        "\n",
        "  # Calculate the average Dice score over the validation dataset\n",
        "  average_dice = np.mean(dice_s)\n",
        "  print(f\"Average Dice Score: {average_dice:.4f}\")\n",
        "\n",
        "  # Calculate the average F1 score over the validation dataset\n",
        "  average_f1 = np.mean(f1_s)\n",
        "  print(f\"Average F1 Score: {average_f1:.4f}\")\n",
        "\n",
        "  # Calculate the average IoU score over the validation dataset\n",
        "  average_iou = np.mean(iou_s)\n",
        "  print(f\"Average IoU Score: {average_iou:.4f}\")\n",
        "\n",
        "  # Calculate average metrics for the epoch\n",
        "  iou_scores.append(average_iou)\n",
        "  precision_scores.append(average_precision)\n",
        "  recall_scores.append(np.mean(recall_s))\n",
        "  f1_val.append(average_f1)\n",
        "  dice_scores.append(average_dice)\n",
        "\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "brHjiIH2u54Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TED9OGwvWACX"
      },
      "outputs": [],
      "source": [
        "def save_predictions_as_imgs(loader,model,folder=\"saved_images/\",device = \"cuda\"):\n",
        "  model.eval()\n",
        "  for idx, (x,y) in enumerate(loader):\n",
        "    x = x.to(device = device)\n",
        "    y = y.unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      preds = torch.sigmoid(model(x))\n",
        "      preds = (preds > 0.5).float()\n",
        "\n",
        "    torchvision.utils.save_image(preds,f\"{folder}/pred_{idx}.jpg\")\n",
        "    torchvision.utils.save_image(y,f\"{folder}/{idx}.jpg\")\n",
        "\n",
        "  model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtoCrNVZZuJm"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,padding=1,kernel_size = 3,stride = 1,with_nonlinearity=True):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride)\n",
        "    self.bn = nn.BatchNorm2d(out_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.with_nonlinearity = with_nonlinearity\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.conv(x)\n",
        "    x = self.bn(x)\n",
        "    if self.with_nonlinearity:\n",
        "        x = self.relu(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz-QIGPCe7q4"
      },
      "outputs": [],
      "source": [
        "class Bridge(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.bridge = nn.Sequential(\n",
        "        ConvBlock(in_channels, out_channels),\n",
        "        ConvBlock(out_channels, out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bridge(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KL0PKpYXfaLE"
      },
      "outputs": [],
      "source": [
        "class UpBlockForUNetWithResNet50(nn.Module):\n",
        "  \"\"\"\n",
        "    Up block that encapsulates one up-sampling step which consists of Upsample -> ConvBlock -> ConvBlock\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, up_conv_in_channels=None, up_conv_out_channels=None,upsampling_method=\"conv_transpose\"):\n",
        "    super().__init__()\n",
        "    if up_conv_in_channels == None:\n",
        "        up_conv_in_channels = in_channels\n",
        "\n",
        "    if up_conv_out_channels == None:\n",
        "        up_conv_out_channels = out_channels\n",
        "\n",
        "    if upsampling_method == \"conv_transpose\":\n",
        "        self.upsample = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    self.conv_block_1 = ConvBlock(in_channels, out_channels)\n",
        "    self.conv_block_2 = ConvBlock(out_channels, out_channels)\n",
        "\n",
        "  def forward(self, up_x, down_x):\n",
        "    \"\"\"\n",
        "      :param up_x: this is the output from the previous up block\n",
        "      :param down_x: this is the output from the down block\n",
        "      :return: upsampled feature map\n",
        "    \"\"\"\n",
        "\n",
        "    x = self.upsample(up_x)\n",
        "\n",
        "    if x.shape != down_x.shape:\n",
        "      x = TF.resize(x,size = down_x.shape[2:])\n",
        "\n",
        "    x = torch.cat([x, down_x], 1)\n",
        "    x = self.conv_block_1(x)\n",
        "    x = self.conv_block_2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puv-yRC3guv_"
      },
      "outputs": [],
      "source": [
        "class UNetWithResnet50Encoder(nn.Module):\n",
        "  DEPTH = 6\n",
        "\n",
        "  def __init__(self, n_classes=1):\n",
        "    super().__init__()\n",
        "    resnet = torchvision.models.resnet.resnet50(pretrained=True)\n",
        "    for param in resnet.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    down_blocks = []\n",
        "    up_blocks = []\n",
        "\n",
        "    self.input_block = nn.Sequential(*list(resnet.children()))[:3]\n",
        "    self.input_pool = list(resnet.children())[3]\n",
        "\n",
        "    for bottleneck in list(resnet.children()):\n",
        "        if isinstance(bottleneck, nn.Sequential):\n",
        "            down_blocks.append(bottleneck)\n",
        "\n",
        "    self.down_blocks = nn.ModuleList(down_blocks)\n",
        "    self.bridge = Bridge(2048, 2048)\n",
        "\n",
        "    up_blocks.append(UpBlockForUNetWithResNet50(2048, 1024))\n",
        "    up_blocks.append(UpBlockForUNetWithResNet50(1024, 512))\n",
        "    up_blocks.append(UpBlockForUNetWithResNet50(512, 256))\n",
        "    up_blocks.append(UpBlockForUNetWithResNet50(in_channels=128 + 64, out_channels=128,up_conv_in_channels=256, up_conv_out_channels=128))\n",
        "    up_blocks.append(UpBlockForUNetWithResNet50(in_channels=64 + 3, out_channels=64,up_conv_in_channels=128, up_conv_out_channels=64))\n",
        "\n",
        "    self.up_blocks = nn.ModuleList(up_blocks)\n",
        "    self.out = nn.Conv2d(64, n_classes, kernel_size=1, stride=1)\n",
        "\n",
        "  def forward(self, x, with_output_feature_map=False):\n",
        "    pre_pools = dict()\n",
        "    pre_pools[f\"layer_0\"] = x\n",
        "\n",
        "    x = self.input_block(x)\n",
        "    pre_pools[f\"layer_1\"] = x\n",
        "\n",
        "    x = self.input_pool(x)\n",
        "\n",
        "    for i, block in enumerate(self.down_blocks, 2):\n",
        "        x = block(x)\n",
        "        if i == (UNetWithResnet50Encoder.DEPTH - 1):\n",
        "            continue\n",
        "        pre_pools[f\"layer_{i}\"] = x\n",
        "\n",
        "    x = self.bridge(x)\n",
        "\n",
        "    for i, block in enumerate(self.up_blocks, 1):\n",
        "        key = f\"layer_{UNetWithResnet50Encoder.DEPTH - 1 - i}\"\n",
        "        x = block(x, pre_pools[key])\n",
        "\n",
        "    output_feature_map = x\n",
        "\n",
        "    x = self.out(x)\n",
        "    del pre_pools\n",
        "\n",
        "    if with_output_feature_map:\n",
        "        return x, output_feature_map\n",
        "    else:\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNetWithResnet50Encoder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll1FaJ9o6Ktp",
        "outputId": "3d5f81cb-ecec-416f-c106-4b1c9e87888c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 156MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model,input_size = (3,256,256))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Vo4HqWFT6PMo",
        "outputId": "9c7e2b24-f565-439b-875b-f861e4ac04e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-82073215ec8f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-4fc0b8a72b32>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, with_output_feature_map)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpre_pools\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"layer_0\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mpre_pools\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"layer_1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m             for hook_id, hook in (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = torchvision.models.resnet.resnet50(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlUpR9MoeyZi",
        "outputId": "e96c3c1e-2ec8-4d23-babe-773cba929d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 124MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet"
      ],
      "metadata": {
        "id": "5pK4uqXj4msy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(resnet,input_size = (3,256,256))"
      ],
      "metadata": {
        "id": "JgNBIGPq4YQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr4t6uGyUA0j",
        "outputId": "488ab2d0-1fc8-42c2-c777-6b741883547d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-0f2a72c25e05>:12: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-poster')\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from numpy import random\n",
        "\n",
        "import numpy as np\n",
        "from mpl_toolkits import mplot3d\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-poster')\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 1e-4\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 10\n",
        "NUM_WORKERS = 2\n",
        "train_val = 0.8\n",
        "IMAGE_HEIGHT = 210\n",
        "IMAGE_WIDTH = 210\n",
        "train_valid = 0.8\n",
        "\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = True\n",
        "\n",
        "TRAIN_IMG_DIR = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/new_complete_images/train_img'\n",
        "TRAIN_MASK_DIR = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/new_complete_images/train_mask'\n",
        "val_dir = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/new_complete_images/val_img'\n",
        "val_mask_dir = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/new_complete_images/val_mask'\n",
        "saved_folder = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/new_complete_images/saved_images'\n",
        "\n",
        "# TRAIN_IMG_DIR = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/Updated_Annotated_data/1 btp/1/images_/train_images'\n",
        "# TRAIN_MASK_DIR = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/Updated_Annotated_data/1 btp/1/images_/train_masks'\n",
        "# val_dir = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/new_complete_images/val_img'\n",
        "# val_mask_dir = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/Updated_Annotated_data/1 btp/1/images_/valid_masks'\n",
        "# saved_folder = '/content/gdrive/MyDrive/Summer Project: Slum Prediction Using Deep Learning/Data/Updated_Annotated_data/1 btp/1/images_/saved_folder'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhdaEFE35Pzn"
      },
      "outputs": [],
      "source": [
        "def train_fn(epoch,num_epochs,loader,model,optimizer,loss_fn,scaler,iou_train,precision_train,recall_train,f1_train,loss_train,accuracy_train):\n",
        "  total_loss = 0.0\n",
        "  total_iou = 0.0\n",
        "  total_precision = 0.0\n",
        "  total_recall = 0.0\n",
        "  total_f1 = 0.0\n",
        "  total_accuracy = 0.0\n",
        "\n",
        "  length = len(loader);\n",
        "  loop = tqdm(loader)\n",
        "\n",
        "  for batch_idx, (data,targets) in enumerate(loop):\n",
        "    data = data.to(device = device)\n",
        "    targets = targets.float().unsqueeze(1).to(device = device)\n",
        "    targ = targets\n",
        "\n",
        "    # forward\n",
        "    with torch.cuda.amp.autocast():\n",
        "      predictions = torch.sigmoid(model(data))\n",
        "\n",
        "      print(np.unique(data))\n",
        "      print(np.unique(predictions))\n",
        "\n",
        "      loss = loss_fn(predictions,targets)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      # convert model outputs to binary mask using sigmoid and threshold\n",
        "      predicted_masks = predictions\n",
        "      predicted_masks = (predicted_masks > 0.5).float()\n",
        "\n",
        "      preds = predicted_masks.cpu().numpy()\n",
        "      yy = targ.cpu().numpy()\n",
        "\n",
        "      # Calculate the intersection and union of the binary masks\n",
        "      intersection = np.sum(preds * yy)\n",
        "      union = np.sum(np.logical_or(preds, yy))\n",
        "      iou = intersection / union\n",
        "\n",
        "      precision = precision_score(yy.flatten(), preds.flatten())\n",
        "      recall = recall_score(yy.flatten(), preds.flatten())\n",
        "      f1 = f1_score(yy.flatten(), preds.flatten())\n",
        "      accuracy = np.mean(yy.flatten() == preds.flatten())\n",
        "\n",
        "      total_iou += iou\n",
        "      total_precision += precision\n",
        "      total_recall += recall\n",
        "      total_f1 += f1\n",
        "      total_accuracy += accuracy\n",
        "\n",
        "    # backward\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # update tqdm loop\n",
        "    loop.set_postfix(loss = loss.item())\n",
        "\n",
        "  average_loss = total_loss / length\n",
        "  average_iou = total_iou / length\n",
        "  average_precision = total_precision / length\n",
        "  average_recall = total_recall / length\n",
        "  average_f1 = total_f1 / length\n",
        "  average_accuracy = accuracy /length\n",
        "\n",
        "  # Append metrics and losses to lists for plotting\n",
        "  loss_train.append(average_loss)\n",
        "  iou_train.append(average_iou)\n",
        "  precision_train.append(average_precision)\n",
        "  recall_train.append(average_recall)\n",
        "  f1_train.append(average_f1)\n",
        "  accuracy_train.append(average_accuracy)\n",
        "\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "          f\"Loss: {average_loss:.4f}, \"\n",
        "          f\"IoU: {average_iou:.4f}, \"\n",
        "          f\"Precision: {average_precision:.4f}, \"\n",
        "          f\"Recall: {average_recall:.4f}, \"\n",
        "          f\"F1: {average_f1:.4f},\"\n",
        "          f\"Accuracy: {average_accuracy:4f}\"\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRT32f2S6Ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "outputId": "fd7b6f43-552e-40a7-f6bd-d3173933d9b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 225MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "  0%|          | 0/178 [02:25<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5c9d148af12e>\u001b[0m in \u001b[0;36m<cell line: 148>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-5c9d148af12e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miou_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-6f9b8c9078be>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(epoch, num_epochs, loader, model, optimizer, loss_fn, scaler, iou_train, precision_train, recall_train, f1_train, loss_train, accuracy_train)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 171, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "  train_transform = A.Compose(\n",
        "    [\n",
        "      A.Resize(height = IMAGE_HEIGHT,width = IMAGE_WIDTH),\n",
        "      A.RandomCrop(height = IMAGE_HEIGHT,width=IMAGE_WIDTH),\n",
        "      A.Rotate(limit = 35,p=1.0),\n",
        "      A.HorizontalFlip(p=0.5),\n",
        "      A.VerticalFlip(p=0.1),\n",
        "      A.Normalize(\n",
        "          mean=[0.0,0.0,0.0],\n",
        "          std = [1.0,1.0,1.0],\n",
        "          max_pixel_value = 1.0\n",
        "      ),\n",
        "      ToTensorV2(),\n",
        "    ], is_check_shapes=False\n",
        "  )\n",
        "\n",
        "  val_transform = A.Compose(\n",
        "    [\n",
        "     A.Resize(height = IMAGE_HEIGHT,width = IMAGE_WIDTH),\n",
        "     A.RandomCrop(height = IMAGE_HEIGHT,width=IMAGE_WIDTH),\n",
        "     A.Normalize(\n",
        "      mean=[0.0,0.0,0.0],\n",
        "      std = [1.0,1.0,1.0],\n",
        "      max_pixel_value = 1.0\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "  ], is_check_shapes=False\n",
        "  )\n",
        "\n",
        "  train_loader,val_loader = get_loaders(\n",
        "      TRAIN_IMG_DIR,\n",
        "      TRAIN_MASK_DIR,\n",
        "      val_dir,\n",
        "      val_mask_dir,\n",
        "      BATCH_SIZE,\n",
        "      train_transform,\n",
        "      val_transform,\n",
        "      train_val,\n",
        "      NUM_WORKERS,\n",
        "      PIN_MEMORY,\n",
        "  )\n",
        "\n",
        "  model = UNetWithResnet50Encoder().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(),lr = LEARNING_RATE)\n",
        "  scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n",
        "\n",
        "  # if LOAD_MODEL:\n",
        "    # load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"),model)\n",
        "    # load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"),model,optimizer=optimizer)\n",
        "\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "  # Initialize lists to store metric values for each epoch for validation dataset\n",
        "  iou_scores = []\n",
        "  precision_scores = []\n",
        "  recall_scores = []\n",
        "  f1_val = []\n",
        "  dice_scores = []\n",
        "  accuracy_vals = []\n",
        "\n",
        "  # Initialize lists to store metric values for each epoch for training dataset\n",
        "  iou_train = []\n",
        "  precision_train = []\n",
        "  recall_train = []\n",
        "  f1_train = []\n",
        "  loss_train = []\n",
        "  accuracy_train = []\n",
        "\n",
        "  # Number of patience for early stopping\n",
        "  patience = 10\n",
        "  # best_val_loss = float('inf')\n",
        "  best_val_loss = torch.tensor(float('inf'))\n",
        "  counter = 0\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    scheduler.step()\n",
        "    train_fn(epoch,NUM_EPOCHS,train_loader,model,optimizer,loss_fn,scaler,iou_train,precision_train,recall_train,f1_train,loss_train,accuracy_train)\n",
        "\n",
        "    # save model\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "    # check_accuracy\n",
        "    val_loss = 0.0\n",
        "    check_accuracy(loss_fn,val_loader,model,accuracy_vals,dice_scores,iou_scores,precision_scores,recall_scores,f1_val,val_loss,device = DEVICE)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    # print some examples to the folder\n",
        "    save_predictions_as_imgs(\n",
        "        val_loader,model,folder=saved_folder,device = DEVICE\n",
        "    )\n",
        "  print(\"Training finished\")\n",
        "\n",
        "  # Plot metrics and losses\n",
        "  # Plot for Training dataset\n",
        "  epochs = np.arange(1, NUM_EPOCHS + 1)\n",
        "  plt.figure(figsize=(12, 8))\n",
        "\n",
        "  plt.plot(epochs, loss_train, label='Train_Loss')\n",
        "  plt.plot(epochs, iou_train, label='Train_IoU')\n",
        "  plt.plot(epochs, precision_train, label='Train_Precision')\n",
        "  plt.plot(epochs, recall_train, label='Train_Recall')\n",
        "  plt.plot(epochs, f1_train, label='Train_F1')\n",
        "  # plt.plot(epochs, accuracy_train, label='Train_Accuracy')\n",
        "\n",
        "  plt.xlabel('Train_Epoch')\n",
        "  plt.ylabel('Score / Loss')\n",
        "  plt.title('Training Metrics and Losses Over Epochs')\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  # PLot validation metrics\n",
        "  plt.figure(figsize=(12, 8))\n",
        "\n",
        "  plt.plot(epochs, iou_scores, label='validation_IoU')\n",
        "  plt.plot(epochs, precision_scores, label='validation_Precision')\n",
        "  plt.plot(epochs, recall_scores, label='validation_Recall')\n",
        "  plt.plot(epochs, f1_val, label='validation_F1')\n",
        "  plt.plot(epochs, dice_scores, label='validation_Dice')\n",
        "  plt.plot(epochs,accuracy_vals,label = \"validation_Accuracy\")\n",
        "\n",
        "  plt.xlabel('validation_Epoch')\n",
        "  plt.ylabel('Score')\n",
        "  plt.title('Validation Metrics Over Epochs')\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  summary(model,input_size = (3,IMAGE_HEIGHT,IMAGE_WIDTH))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k68LwV2LYeVs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
